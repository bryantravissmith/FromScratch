{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Trees are a [non-parametric](https://en.wikipedia.org/wiki/Nonparametric_statistics) [supervised machine learning](https://en.wikipedia.org/wiki/Supervised_learning) algorithm that takes a set of training data and constructs a set of regions in the space of features that is then used to make predictions. These predictions can be continuous values (Decision Tree Regression), or classification (Decision Tree Classification).\n",
    "\n",
    "[caption id=\"attachment_2353\" align=\"aligncenter\" width=\"1000\"]<img class=\"wp-image-2353 size-full\" src=\"https://bryantravissmithdotcom.files.wordpress.com/2016/11/decision-tree-algorithm-page-1.png\" alt=\"Decision Tree Process\" width=\"1000\" height=\"580\" /> High Level Decision Tree Process[/caption]\n",
    "\n",
    " \n",
    "\n",
    "Let's imagine we have a data set with a lot of features and how much student debt individuals have. We could just take the average of the entire dataset, say $25,000.00, and use that as a prediction. A better prediction could be developed by dividing the dataset up using the level of college completed: None, Some, Undergraduate, and Graduate. We can then take the average of subset/region:\n",
    "\n",
    "1. None: \\$0\n",
    "2. Some: \\$2000\n",
    "3. Undergraduate: \\$16000\n",
    "4. Graduate: \\$42000\n",
    "\n",
    "Now when we score a person, we use their level of college completed to make the prediction. The idea is that the results are improved over the $25,000 average estimate of the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"aligncenter size-full wp-image-2356\" src=\"https://bryantravissmithdotcom.files.wordpress.com/2016/11/decision-tree-algorithm-page-2.png\" alt=\"Student Loan Example\" width=\"768\" height=\"529\" />\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Why Implement It Ourselves?\n",
    "\n",
    "The reason why I want to implement machine learning algorithm from scratch is I have a better intuition for the assumptions and the trade off in the implementations of standard libraries. For instance, before I did this for the first time I did not appreciate that both [sklearn](http://scikit-learn.org/stable/) (python) and [rpart](https://cran.r-project.org/web/packages/rpart/index.html) (R) have an implementation of the [CART](https://en.wikipedia.org/wiki/Decision_tree_learning?#Types) algorithm for decision trees, but there are equally valid decision tree algorithms that would produce different results. This implementation is inherently [greedy](https://en.wikipedia.org/wiki/Greedy_algorithm). The issue with greedy algorithms is that making locally best splits or regions may not make the globally best set of regions. This is what makes finding the best Decision Tree model [np-hard](https://en.wikipedia.org/wiki/NP-hardness). If you really get into it, finding the best tree becomes an [optimal stopping](https://en.wikipedia.org/wiki/Optimal_stopping) problem and brushes into [Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning) and [Artifical Intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) style searching.\n",
    "\n",
    "On a more practical side, there have been a number of times in the iterative approach of model-building where understanding the model assumptions have allowed me to exclude features the violate assumptions, or create new features that are more easily expressed and represented by the model. Both of these have consistently lead to better out of sample performance.\n",
    "\n",
    "Finally, the best reason is I get pleasure and satisfaction from doing this. I think algorithms are a powerful framework and a fun way to think about problems solving and doing things from scratch help improve about execution and understanding of them.\n",
    "\n",
    "## Metrics - Measures for Improvement\n",
    "\n",
    "I made up the student debt example. It makes sense in my mind, but we need to define an algorithm that can create the regions from any dataset. Most machine learning algorithms are optimization algorithms that improve some measure/metric on each iteration. Decision trees are no different in that respect. To define the algorithm, we need to define the measures.\n",
    "\n",
    "### Classification Metrics for Decision Trees\n",
    "\n",
    "#### Gini Impurity\n",
    "\n",
    "Gini Impurity is a very intuitive metric to try to improve because it is a measure of the misclassification rate. If the fraction of the data set that has outcome class $  c  $ is $  f_c  $, we can make predictions for each data point as being class $  c  $, $  f_c  $ of the time. The error rate for a class is the probability of getting the class times the probability of getting it wrong.\n",
    "\n",
    "$$  Error_c = f_c * \\left( 1 - f_c \\right)  $$\n",
    "\n",
    "The Gini Impurity is the sum of these error rates for all classes.\n",
    "\n",
    "$$  Gini \\ Impurity = \\sum_c Error_c = \\sum_c f_c * \\left( 1 - f_c \\right)  $$\n",
    "$$  Gini \\ Impurity = \\sum_c f_c - \\sum _c f^2_c  $$\n",
    "$$  Gini \\ Impurity = 1 - \\sum _c f^2_c  $$\n",
    "\n",
    "If we divide the data set in regions that improve the weighted sum of the Gini Impurity, the misclassification rate will be reduced.\n",
    "\n",
    "#### Gini Impurity for a Coin Flip\n",
    "\n",
    "A coin flip is about as simple as a process there is to calculate the Gini Impurity for. We have a coin with probability of head, $  p_H  $. The Gini Impurity is then\n",
    "$$  Gini \\ Impurity = 1 - p_H^2 - \\left(1-p_H\\right)^2 = 2*p_H*\\left(1-p_H\\right)  $$\n",
    "A plot of the this is below for different values of $  p_H  $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABRIAAAKaCAYAAABC5aWJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzs3XmcnvOh9/HvJUGs0doJCQ2JJUFyEFvtYivlqDZFtbaj\n6hwnj/ZoTz110Fa16K7ao6W2aFWr6KM0KGqXoC0RqvataMWSIOR6/vhlXpNE0kvGJPcs7/frdb9m\ncs19z/wSZuaez/yWqq7rAAAAAAD8M4u0egAAAAAAQNcnJAIAAAAAjYREAAAAAKCRkAgAAAAANBIS\nAQAAAIBGQiIAAAAA0EhIBAAAAAAaCYkAAAAAQCMhEQAAAABoJCQCAAAAAI3mOyRWVbVNVVWXV1X1\nVFVVM6qq2utdPGa7qqomVFX1elVVD1ZVdXDHhgsAAAAAtEJHZiQuleSeJJ9JUjfduaqqQUmuTHJt\nko2SfDvJ2VVV7dyBjw0AAAAAtEBV140tcN4PrqoZST5c1/Xl/+Q+pybZra7r4bNcG5ekf13Xu3f4\ngwMAAAAAC83C2CNxVJLxc1y7OskWC+FjAwAAAACdoO9C+BirJHlujmvPJVm2qqrF67p+Y84HVFW1\nfJLRSR5N8voCHyEAAAAA9Cz9kgxKcnVd1y92xjtcGCFxbqqZL+e1rnp0kgsX0lgAAAAAoKc6IMlF\nnfGOFkZIfDbJynNcWynJy3VdvzmPxzyaJBdccEHWW2+9BTg0oBXGjh2bb37zm60eBrAA+PyGnsvn\nN/RcPr+hZ5o0aVIOPPDAZGZn6wwLIyTemmS3Oa7tMvP6vLyeJOutt15GjBixoMYFtEj//v19bkMP\n5fMbei6f39Bz+fyGHq/Ttg2c78NWqqpaqqqqjaqq2njmpbVn/nmNmW8/paqqn87ykLOSfKCqqlOr\nqhpSVdVRSfZLcsZ7Hj0AAAAAsFB05NTmf0lyd5IJKXscnp5kYpITZ759lSRrtN25rutHk+yRZKck\n9yQZm+TQuq7nPMkZAAAAAOii5ntpc13XN+SfBMi6rj81j8eMnN+PBQAAAAB0DR2ZkQjwnowZM6bV\nQwAWEJ/f0HP5/Iaey+c38G5VdV23egzvUFXViCQTJkyYYMNXAAAAAJhPEydOzMiRI5NkZF3XEzvj\nfZqRCAAAAAA0EhIBAAAAgEZCIgAAAADQSEgEAAAAABoJiQAAAABAIyERAAAAAGgkJAIAAAAAjYRE\nAAAAAKCRkAgAAAAANBISAQAAAIBGQiIAAAAA0EhIBAAAAAAaCYkAAAAAQCMhEQAAAABoJCQCAAAA\nAI2ERAAAAACgkZAIAAAAADQSEgEAAACARkIiAAAAANBISAQAAAAAGgmJAAAAAEAjIREAAAAAaCQk\nAgAAAACNhEQAAAAAoJGQCAAAAAA0EhIBAAAAgEZCIgAAAADQSEgEAAAAABoJiQAAAABAIyERAAAA\nAGgkJAIAAAAAjYREAAAAAKCRkAgAAAAANBISAQAAAIBGQiIAAAAA0EhIBAAAAAAaCYkAAAAAQCMh\nEQAAAABoJCQCAAAAAI2ERAAAAACgkZAIAAAAADQSEgEAAACARkIiAAAAANBISAQAAAAAGgmJAAAA\nAEAjIREAAAAAaCQkAgAAAACNhEQAAAAAoJGQCAAAAAA0EhIBAAAAgEZCIgAAAADQSEgEAAAAABoJ\niQAAAABAIyERAAAAAGgkJAIAAAAAjYREAAAAAKCRkAgAAAAANBISAQAAAIBGQiIAAAAA0EhIBAAA\nAAAaCYkAAAAAQCMhEQAAAABoJCQCAAAAAI2ERAAAAACgkZAIAAAAADQSEgEAAACARkIiAAAAANBI\nSAQAAAAAGgmJAAAAAEAjIREAAAAAaCQkAgAAAACNhEQAAAAAoJGQCAAAAAA0EhIBAAAAgEZCIgAA\nAADQSEgEAAAAABoJiQAAAABAIyERAAAAAGgkJAIAAAAAjYREAAAAAKCRkAgAAAAANBISAQAAAIBG\nQiIAAAAA0EhIBAAAAAAaCYkAAAAAQCMhEQAAAABoJCQCAAAAAI2ERAAAAACgkZAIAAAAADQSEgEA\nAACARkIiAAAAANBISAQAAAAAGgmJAAAAAEAjIREAAAAAaCQkAgAAAACNhEQAAAAAoJGQCAAAAAA0\nEhIBAAAAgEZCIgAAAADQSEgEAAAAABoJiQAAAABAIyERAAAAAGgkJAIAAAAAjYREAAAAAKCRkAgA\nAAAANBISAQAAAIBGQiIAAAAA0EhIBAAAAAAaCYkAAAAAQCMhEQAAAABoJCQCAAAAAI06FBKrqvpM\nVVWPVFU1raqq26qq2rTh/v9ZVdUDVVVNrarq8aqqzqiqavGODRkAAAAAWNjmOyRWVfXRJKcnOSHJ\nJknuTXJ1VVUrzOP+H09yysz7D01ySJKPJvlKB8cMAAAAACxkHZmRODbJD+u6Pq+u6weSHJlkakog\nnJstkvyhruuf1XX9eF3X45OMS7JZh0YMAAAAACx08xUSq6paNMnIJNe2Xavruk4yPiUYzs0tSUa2\nLX+uqmrtJLsn+U1HBgwAAAAALHx95/P+KyTpk+S5Oa4/l2TI3B5Q1/W4mcue/1BVVTXz8WfVdX3q\n/A4WAAAAAGiN+Q2J81Ilqef6hqraLsl/pyyBviPJ4CTfqarqmbquv/zP3unYsWPTv3//2a6NGTMm\nY8aM6YwxAwAAAEC3N27cuIwbN262a1OmTOn0j1OVlcnv8s5lafPUJP9a1/Xls1w/N0n/uq73mctj\nbkxya13Xx81y7YCUfRaXnsfHGZFkwoQJEzJixIh3PT4AAAAAIJk4cWJGjhyZJCPrup7YGe9zvvZI\nrOt6epIJSXZsuzZzufKOKXshzs2SSWbMcW3GzIdW8/PxAQAAAIDW6MjS5jOS/LSqqgkpS5XHpsTC\nc5OkqqrzkjxZ1/V/z7z/FUnGVlV1T5Lbk6yT5KQkv67nZzokAAAAANAy8x0S67r++czDU05KsnKS\ne5KMruv6+Zl3GZDkrVkecnLKDMSTk6ye5Pkklyc5/j2MGwAAAABYiDp02Epd12cmOXMeb9thjj+3\nRcSTO/KxAAAAAIDWm689EgEAAACA3klIBAAAAAAaCYkAAAAAQCMhEQAAAABoJCQCAAAAAI2ERAAA\nAACgkZAIAAAAADQSEgEAAACARkIiAAAAANBISAQAAAAAGgmJAAAAAEAjIREAAAAAaCQkAgAAAACN\nhEQAAAAAoJGQCAAAAAA0EhIBAAAAgEZCIgAAAADQSEgEAAAAABoJiQAAAABAIyERAAAAAGgkJAIA\nAAAAjYREAAAAAKCRkAgAAAAANBISAQAAAIBGQiIAAAAA0EhIBAAAAAAaCYkAAAAAQCMhEQAAAABo\nJCQCAAAAAI2ERAAAAACgkZAIAAAAADQSEgEAAACARkIiAAAAANBISAQAAAAAGgmJAAAAAEAjIREA\nAAAAaCQkAgAAAACNhEQAAAAAoJGQCAAAAAA0EhIBAAAAgEZCIgAAAADQSEgEAAAAABoJiQAAAABA\nIyERAAAAAGgkJAIAAAAAjYREAAAAAKCRkAgAAAAANBISAQAAAIBGQiIAAAAA0EhIBAAAAAAaCYkA\nAAAAQCMhEQAAAABoJCQCAAAAAI2ERAAAAACgkZAIAAAAADQSEgEAAACARkIiAAAAANBISAQAAAAA\nGgmJAAAAAEAjIREAAAAAaCQkAgAAAACNhEQAAAAAoJGQCAAAAAA0EhIBAAAAgEZCIgAAAADQSEgE\nAAAAABoJiQAAAABAIyERAAAAAGgkJAIAAAAAjYREAAAAAKCRkAgAAAAANBISAQAAAIBGQiIAAAAA\n0EhIBAAAAAAaCYkAAAAAQCMhEQAAAABoJCQCAAAAAI2ERAAAAACgkZAIAAAAADQSEgEAAACARkIi\nAAAAANBISAQAAAAAGgmJAAAAAEAjIREAAAAAaCQkAgAAAACNhEQAAAAAoJGQCAAAAAA0EhIBAAAA\ngEZCIgAAAADQSEgEAAAAABoJiQAAAABAIyERAAAAAGgkJAIAAAAAjYREAAAAAKCRkAgAAAAANBIS\nAQAAAIBGQiIAAAAA0EhIBAAAAAAaCYkAAAAAQCMhEQAAAABoJCQCAAAAAI2ERAAAAACgkZAIAAAA\nADQSEgEAAACARkIiAAAAANBISAQAAAAAGgmJAAAAAEAjIREAAAAAaCQkAgAAAACNhEQAAAAAoJGQ\nCAAAAAA0EhIBAAAAgEZCIgAAAADQqEMhsaqqz1RV9UhVVdOqqrqtqqpNG+7fv6qq71dV9fTMxzxQ\nVdWuHRsyAAAAALCw9Z3fB1RV9dEkpyc5IskdScYmubqqqnXrun5hLvdfNMn4JM8m2TfJ00kGJnnp\nPYwbAAAAAFiI5jskpoTDH9Z1fV6SVFV1ZJI9khyS5Otzuf+hSZZLMqqu67dnXnu8Ax8XAAAAAGiR\n+VraPHN24cgk17Zdq+u6TplxuMU8HvahJLcmObOqqmerqvpTVVVfqKrK/owAAAAA0E3M74zEFZL0\nSfLcHNefSzJkHo9ZO8kOSS5IsluSdZKcOfP9fHk+Pz4AAAAA0AIdWdo8N1WSeh5vWyQlNB4xc/bi\n3VVVrZ7ks2kIiWPHjk3//v1nuzZmzJiMGTPmvY8YAAAAAHqAcePGZdy4cbNdmzJlSqd/nKq0vXd5\n57K0eWqSf63r+vJZrp+bpH9d1/vM5TG/T/JmXde7zHJt1yS/SbJ4XddvzeUxI5JMmDBhQkaMGPHu\n/zYAAAAAQCZOnJiRI0cmyci6rid2xvucr30K67qenmRCkh3brlVVVc388y3zeNjNSQbPcW1Ikmfm\nFhEBAAAAgK6nIweenJHkiKqqPlFV1dAkZyVZMsm5SVJV1XlVVX11lvv/IMnyVVV9u6qqdaqq2iPJ\nF5J8770NHQAAAABYWOZ7j8S6rn9eVdUKSU5KsnKSe5KMruv6+Zl3GZDkrVnu/2RVVbsk+WaSe5M8\nNfP1r7/HsQMAAAAAC0mHDlup6/rMlJOX5/a2HeZy7fYkW3bkYwEAAAAArdeRpc0AAAAAQC8jJAIA\nAAAAjYREAAAAAKCRkAgAAAAANBISAQAAAIBGQiIAAAAA0EhIBAAAAAAaCYkAAAAAQCMhEQAAAABo\nJCQCAAAAAI2ERAAAAACgkZAIAAAAADQSEgEAAACARkIiAAAAANBISAQAAAAAGgmJAAAAAEAjIREA\nAAAAaCQkAgAAAACNhEQAAAAAoJGQCAAAAAA0EhIBAAAAgEZCIgAAAADQSEgEAAAAABoJiQAAAABA\nIyERAAAAAGgkJAIAAAAAjYREAAAAAKCRkAgAAAAANBISAQAAAIBGQiIAAAAA0EhIBAAAAAAaCYkA\nAAAAQCMhEQAAAABoJCQCAAAAAI2ERAAAAACgkZAIAAAAADQSEgEAAACARkIiAAAAANBISAQAAAAA\nGgmJAAAAAEAjIREAAAAAaCQkAgAAAACNhEQAAAAAoJGQCAAAAAA0EhIBAAAAgEZCIgAAAADQSEgE\nAAAAABoJiQAAAABAIyERAAAAAGgkJAIAAAAAjYREAAAAAKCRkAgAAAAANBISAQAAAIBGQiIAAAAA\n0EhIBAAAAAAa9W31AAAAWLhmzEimT393t7femv/336dPsuiizbfFFksW8WttAIBuQ0gEAOiiZsxI\nXnklmTJl9ttLL7W//tpr5TZ16jtvc7s+bVp5v13FIosk/folSy75zttSS8392lJLJf37z/u27LIl\nZgIA0LmERACABayuS9R74YVye/75ub/+wgslEraFwldeKY+dm759SzRbeum5B7cVV5x7hOvX793N\nFmy79e2bVNX8/V3ffvufz3J8883Z//zGG/MOn6+9lvz977Nfa4ur06fPexzLLNMeFpdbLll++fJv\nssIK5Ta315dZZv7+rgAAvY2QCADQQW+8kTz7bPLMM+X29NOzv/7ss+2B8PXX3/n4pZaaPWituWay\n0UazB7C5zbhbbrlkiSV6d/Sq6/Jv+s9ma8567YUXkj/9qT3evvLKO9/nYou1/7dYeeVktdWSVVct\nt1lfX3XV8u8PANDbCIkAAHPx2mvJ448njz1WXj7+ePLkk7PHwhdfnP0xiy6arLJKe3QaNao9FM45\nA2755cWo96Kqyr/fEkuUf/P59cYb5b/f3GaIPv988txzyYMPJjfcUP5bv/HG7I9fbrnZA+PqqycD\nB5YY3PZy2WU75+8KANBVCIkAQK9T1yUWPfroO2Nh2+uzRsJFFimhaMCAEo6GDHnnDLXVVkve/36H\nh3QXiy9e/puttlrzfes6+cc/2gPynLNPH300ufnm5IknZj+cpn//d8bFWV+uumrvnlUKAHQ/QiIA\n0CPNmFFmED78cPKXv7zz5auvtt93qaXaA89mmyX77Td78FlttbJXIL1TVZVI/P73JxtsMO/7vf12\nWc4+a5Bue3njjeXllCnt919iieQDHyi3wYNnf7nmmv6fAwC6Hk9PAIBuq66Tp55KJk1KJk+ePRT+\n9a/lUI+kzBIcOLAEmlGjkgMOKK+vtVYJNu97n5lhvHd9+pSZq6uvnmyxxdzvM2VKCYqPPlr+X237\n//Xyy5NHHmmf0di3bzJo0OyBcciQZL31yv+zZr4CAK0gJAIAXd706SW2PPBAiYazvmybWbjYYsna\na5fgMnr07AFm4MDydmi1/v2TYcPKbU5vvVWWR885g/aGG5Kf/CSZNq3cb4klSlQcOrSExbaX66xT\nTuUGAFhQhEQAoMt4880SB//0p+TPf24Phg8/3D5Ta7nlSjQZPjzZf//y+nrrldlbffq0dPjwnvTt\nW2bJrrVWsvPOs79txowSGds+J9pC+rXXlv0+kzJLca212sPiBhuUYLn++g72AQA6h5AIACx0bUuS\n//jHEg3bXk6a1B4MBwwoMWT06NlnXa20kmXI9D5ty/MHDiyfE7N68cV3zta99NLktNPaH7vuuiUq\nDh/e/nLgQEukAYD5IyQCAAvUtGklFN57b3s0/OMfk5deKm9fZpkSNrbaKjnyyBI4NtywzDwEmi2/\nfPn82Wqr2a+/+mpy332zB/szzignUCfJ0kvPHhc32ijZeONyHQBgboREAKDTTJ1aguGECe23++8v\np9m2zYoaPjzZZZfZZ0WZYQidb+mlk803L7c2dZ08/fTscfGWW8oejNOnl8/FIUOSkSPLbcSIZJNN\nkmWXbd3fAwDoOoREAKBDXn01ueeeEgsnTiwvJ00qe7ktumgJhaNGJUcdVYLEhhvapw1araraT5be\nbbf262++WaJ/2+fyxIllefTrr5e3r7tuiYptgXGTTcwaBoDeSEgEABq99VZZInn77cltt5WXkyaV\n2U2LL15mFm6zTXLMMe3R0CnJ0H0stlhZ1rzxxskhh5Rrb71VPs9njYuXX15mHiflRPS2GY+jRpWl\n0T7vAaBnExIBgHd49tkSDNui4Z13Jq+9Vk5FHj48+eAHk2OPLdFw/fXLDESgZ+nbt8wsHjYsOfjg\ncu3tt5PJk0tUvPPO8vXhkkvKjMbFFy9fE9rC4qhRyRpr2LoAAHoSIREAerk33ihRYNZw+Nhj5W2r\nrVZiwAknlJcjRyZLLtna8QKt06dP+eXB+usnBx5Yrr3xRtnmoO1ryGWXJd/8ZnnbKqu0R8VRo5JN\nN/U1BAC6MyERAHqZl19Obr01uemmcrvjjrIPWr9+JRTut1/7D/0DBrR6tEBXt/ji7UucjzmmXHvu\nufatEG67Lfnyl8u+qn37lq8z22xTbltvnbz//a0dPwDw7gmJANDDPfdcezS86aZyqvKMGcmKK5Yf\n4r/61fJy440tUQY6x8orJ3vtVW5JWRJ9333JH/5Qvg6NG5ecdlp52wYbtIfFbbYpy6EBgK5JSASA\nHuaRR5IbbmgPhw89VK6vtVb5If2oo8rLdde1dxmwcLTtrzp8ePkaVNdlC4W2r1O//31y1lnlvmuu\n2R4Vt902GTLE1yoA6CqERADo5p5+Orn++uS668rt0UfLD93DhiW77JKcfHKZcbj66q0eKUBRVcmg\nQeV20EHl2vPPt89Y/MMfkosvLjMZV1012WGH9tugQS0cOAD0ckIiAHQzL7xQZu+0hcPJk8v1YcOS\nvfdOtt++nKr8vve1dJgA82XFFZN99im3pOypePPN7V/rLrqozGQcNKg9Km6/fTkUCgBYOIREAOji\nXn45ufHG9h+m7723XF9nnfKD9EknJdttl6y0UkuHCdCpll46GT263JLkH/+Y/WvhT35Srg8dWoLi\nDjuUr4UrrNCyIQNAjyckAkAX8/bbycSJydVXl9utt5Zra6yR7Lhj8n/+T/mh2YEEQG/yvveVWdd7\n713+/Le/tc/OHj8++cEPypLpkSPbA+SoUQ6RAoDOJCQCQBfw9NPJNdeUcPi73yUvvpgss0wJh9/7\nXrLTTskHPuDAAYA2K62U7L9/uSXJE08k115bvo6edVbyla8kyy5bZiq2hcW11mrtmAGguxMSAaAF\nXn+9HCjQFg//9Kf2mTRHHmkmDcD8WmON5JOfLLc5Z3YffXS5ts467VFxu+3K8mkA4N0TEgFgIXns\nseTKK5Pf/KYsx5s2rZxGussuyRe+kOy8s729ADpDnz7JppuW2/HHJ1OmlCXQV19dvg5/73vlFzVb\nb53ssUey557Juuua9Q0ATYREAFhA3n47ue228kPrlVcmf/5z0rdvOVH5xBOTXXdNNtzQD64AC1r/\n/u0nQtd18tBDJSr+9rclNH72s8ngwSUo7rlnss02yWKLtXrUAND1CIkA0Ileeql9xstVV5W9DldY\nocx4OeGEMuuwf/9WjxKg96qqMvtw3XWTf//3ZOrUMlvxyiuTSy5JvvWtskft6NElKu62W9mPEQAQ\nEgHgPXvwweSKK8oPoTfdVGYibrRR2etwzz3L0ro+fVo9SgDmZskl22ci1nVy773tM8k/9alyn803\nb7/P8OFmkgPQe1V1Xbd6DO9QVdWIJBMmTJiQESNGtHo4ADCbuk7uvDP51a/KbfLkpF+/csLynnsm\nu++erLlmq0cJwHv13HNldvmVV5bZ5q++mgwcmHz4w2WZ9NZb+0URAF3XxIkTM3LkyCQZWdf1xM54\nn2YkAsC7MH16cuONJRxedlny1FPJ8ssne+2VfP3ryU47lVktAPQcK6/cfhL0G2+U7wOXXVaWQH/7\n22Xrir32Svbdt/wyqV+/Vo8YABYsIREA5mHq1OSaa0o8vOKK5B//SNZYI/nXf22fidLXd1KAXmHx\nxcs+tzvvnHz3u7PPTP/JT5Klly4z0vfZp7xcdtlWjxgAOp8ffwBgFi+9VJaw/epX5TTPqVOT9ddP\njjqq/HA4YoS9sQB6u0UWKfsmbr55csopyaRJyS9/Wb53jBlTTnzeccfyfWPvvR3WAkDPsUirBwAA\nrTZlSnL++cmHPlR+2DvooOTJJ5Mvfansf3jffcmXv5yMHCkiAjC7qiq/cDr++GTChOTRR8uWF1On\nlkO3Vl21bH/xox8lzz/f6tECwHsjJALQK73ySnLhhe0zRT7xibJ0+bTTkieeSG6/PTnuuGTddVs9\nUgC6k4EDk2OOSX7/++TZZ5Ozziqx8dOfLlFxl12Ss89OXnyx1SMFgPknJALQa7z6anLxxWVT/BVX\nTA48MPnb35KvfS15/PHkD39I/uM/kgEDWj1SAHqCFVdMDj88+d3vkmeeSb7//eStt5J/+7dklVWS\n3XZLzjmn/CILALoDIRGAHu2118rpmh/5SJl5OGZMOXH5K18py89uvTUZO7YcogIAC8pKK5WAeN11\n5fvQt7+dTJuWHHpoOR16jz2S884re/UCQFclJALQ47z1VnLVVWXG4corJ/vvnzzySPI//5P89a9l\n2fKxx5blZwCwsK2ySjnE6/e/L1HxjDOSl19ODj64fN/ad9/k0kuT119v9UgBYHZCIgA9Ql2XQPgf\n/5Gstlqy++5l0/vPfz75y1+Su+5K/uu/krXWavVIAaDdqqsmRx+d3HRTOeirbbuN/fYrwfGww5Lr\nr09mzGj1SAEg6dvqAQDAe/Hgg+XQlIsuKsFw1VXLqcsHHJBssolTlgHoPlZfvWy3MXZs8sAD5fvb\nhRcmP/5xedvHP16+vw0f7vsbAK1hRiIA3c5zz5W9pTbbLBkyJPnmN5Ott07Gjy8nLp9+ejJihB+y\nAOi+hg5NTj45efjh5Oabk733Tn7yk2TjjZNhw5JTTkkee6zVowSgtxESAegWpk1Lxo1Ldt21zMr4\n3OfKEuaf/7yExXPOSXbcMenTp9UjBYDOU1XJlluWE5+feSa58soSEk8+ORk0KPngB5Ozzy57LALA\ngiYkAtBl1XVy223JkUeWJcsf/3jy6qvlh6lnn00uu6ycxrzEEq0eKQAseIsuWk53Hjeu/BLtvPOS\nfv2SI44o+ykedFBy7bX2UwRgwbFHIgBdzlNPJeefn5x7bjJ5crLGGsm//3s5zXLw4FaPDgBab5ll\nSjg86KBySEvb980LLkjWXLN8zzz44OQDH2j1SAHoScxIBKBLeP315Gc/S3bbrfwAdOKJyb/8S/K7\n3yWPPFKWcImIAPBOAwYkX/hCOaDl5puT0aOTb32rfN/cdtuy/ccrr7R6lAD0BEIiAC1T18kddyRH\nHVWWLn/sY2WPp7POKkuXL7gg2Wkn+x4CwLvRtp/ij37U/n10scWSQw8tS58PPji5/npLnwHoOCER\ngIXu739PvvOdZPjwZPPNk8svTz796faZFIcfnvTv3+pRAkD3teSSyQEHlJn9jz5aZizefHOyww7J\nOuuUU5+feabVowSguxESAVgo6jq58cayl9NqqyXHHpsMGZJcdVXy2GPJV79a/gwAdK4110yOPz55\n6KHyvXjrrZOTTip7EO+zT/le/PbbrR4lAN2BkAjAAvX888lppyXrrVf2abrttrL/4ZNPJr/4RbLr\nrpYuA8DCUFXJNtskP/1pmY347W8nf/1rsvvuydprl+/PTzzR6lEC0JUJiQB0uhkzkvHjk49+NFl9\n9eSLX0xGjEiuu66cwnzcccnKK7d6lADQey23XPKZzyT33JPcfnuy887JN76RDBqU7Lln2Xbkrbda\nPUoAuhr7tctQAAAgAElEQVQhEYBO8+yzZc+lddYpP5D88Y/JqacmTz2VXHRRsv32ySK+8wBAl1FV\nyWabJWefnTz9dPKDH5Tv53vvnQwcWJZEP/poq0cJQFfhxzkA3pO2vQ8/+tGy19JJJ5W9l266Kbn/\n/mTs2GSFFVo9SgCgybLLJkcckdx1VzJxYomJ3/lOWfa8555lL0UnPgP0bkIiAB3yyitl1sLw4WXv\nw3vuKXshPv102Xtp663LLAcAoPvZZJPkzDPLXoo/+lFZXbD77mXVwWmnJS++2OoRAtAKQiIA8+X+\n+5Ojjy57Hx59dDJ4cPK73yWTJiXHHJO8732tHiEA0FmWWio57LAyQ/GWW5Ittyx7Hw8YkHzqU8md\nd7Z6hAAsTEIiAI2mT08uuaTscbjBBuW05WOOKXsm/epXyU472fsQAHqyqkq22CI5//zkySeTE05I\nrr++7K+42WbJuecm06a1epQALGh+7ANgnp5+Ovmf/ymbre+/fzm9cdy45PHHk5NPLnsiAgC9y4or\nJp//fPLww+V05+WXL7MTBwxIPve5ch2AnklIBOAdbr89+fjHS0A87bSy2fq995YDVD72sWSxxVo9\nQgCg1fr0ST70oXIIy0MPlZj44x+XfRT32iu59tpyKBsAPUeHQmJVVZ+pquqRqqqmVVV1W1VVm77L\nx32sqqoZVVX9siMfF4AFZ/r05OKLy7KlUaOSO+4oEfGpp9oPVQEAmJvBg9ufN/zv/5btT3baqTx/\nOPtsy54Beor5DolVVX00yelJTkiySZJ7k1xdVdUKDY8bmOQbSW7swDgBWEBefDE55ZRkrbWSMWOS\nJZdMfv3rZPLksg9i//6tHiEA0F0ssURy6KFlJcO11yZrr50ccUTZDuWLXyyhEYDuqyMzEscm+WFd\n1+fVdf1AkiOTTE1yyLweUFXVIkkuSPKlJI90ZKAAdK777itP7AcMSE48Mdl11/Yn/XvtVZYrAQB0\nRFUlO+xQfjn54IPJgQcm3/1uMmhQ2T7ljjtaPUIAOmK+QmJVVYsmGZnk2rZrdV3XScYn2eKfPPSE\nJH+r6/qcjgwSgM4xY0bym98kO++cbLhhcuWVyfHHJ088UZYdWb4MAHS2wYOTb32rnPZ82mllL+bN\nN0+23DL52c/K9ioAdA/zOyNxhSR9kjw3x/XnkqwytwdUVbVVkk8lOWy+RwdAp5g6texzOHRosuee\nyZQpyYUXlv2LvvjFcvoiAMCCtOyyZduUBx9MLrss6devHOK29trJ179enp8A0LX17aT3UyV5x3lc\nVVUtneT8JIfXdf2P+X2nY8eOTf85NucaM2ZMxowZ09FxAvQqzz+ffP/75fb3vyf77pv89KflMJWq\navXoAIDeqE+fZO+9y+2PfyyzFf/v/02+/OWy7coxx5Q9FQF498aNG5dx48bNdm3KAvgNTVVWJr/L\nO5elzVOT/Gtd15fPcv3cJP3rut5njvtvlGRikrdTYmPSPgvy7SRD6rp+x56JVVWNSDJhwoQJGTFi\nxLv/2wCQJHnooeSMM5Jzz00WWSQ55JBk7NjyG38AgK7mmWfKHoo/+EHy6qtlpuJnP5tstFGrRwbQ\nfU2cODEjR45MkpF1XU/sjPc5X0ub67qenmRCkh3brlVVVc388y1zecikJMOSbJxko5m3y5NcN/P1\nJzo0agDm6tZby6zDIUOSX/6yLFt+/PHyxFxEBAC6qlVXTb761fK85bTTkptuSjbeONlll+Saa5L5\nmP8CwALUkVObz0hyRFVVn6iqamiSs5IsmeTcJKmq6ryqqr6aJHVdv1nX9f2z3pK8lOSVuq4n1XX9\nVuf8NQB6rxkzyj5DW21VNi2///7khz9MHnusHKSy/PKtHiEAwLuzzDJlafNf/pKMG5e8+GIyenSy\nySbJBRc4mAWg1eY7JNZ1/fMkxyY5KcndSYYnGV3X9fMz7zIg8zh4BYDOM21aCYZDhyb77FP2G/r1\nr0tIPPzwsoE5AEB31LdvWd58113Jddclq62WHHRQWWFx+unJyy+3eoQAvVNHZiSmrusz67oeVNf1\nEnVdb1HX9V2zvG2Huq4P+SeP/VRd1/t25OMCUJ44n3pqMmhQctRRZe+g225Lbrwx2WuvsiciAEBP\nUFXJ9tsn/+//JX/6U7LTTskXvpCsuWZZefH8883vA4DO48dNgG7i+efLE+Y110y+9KXkwx9OJk9O\nLrkk2XzzVo8OAGDB2nDD5JxzkkceSQ47rJz2PHBgWQr9hN33ARYKIRGgi3viifIEeeDA8oT5sMPK\nE+gf/jAZPLjVowMAWLhWX70cyPLYY8lxxyXnn1+WPB9ySPklKwALjpAI0EU9+GBy6KHJBz5QniAf\nd1x5wnzaaWWfIACA3mz55ZMTTijPj772teS3v03WWy/Zf//k7rtbPTqAnklIBOhi7r67PAEeOjS5\n6qrklFPKE+QTTnACMwDAnJZZJjn22LJi46yzkgkTkhEjkt12S266qdWjA+hZhESALuKmm8oT3hEj\nyhPgs85K/vrX8sR4mWVaPToAgK5t8cWTI44oy5svuih56qnkgx9Mtt66HNZS160eIUD3JyQCtNjv\nf19OI/zgB5Mnn0wuvLA8AT7iiKRfv1aPDgCge+nbNxkzJrn33uSKK5K330722CPZdNPk8ssFRYD3\nQkgEaIG6Tq69Ntl22xIRp0xJfvWr8oT34x8vT4ABAOi4qkr23DO55ZbyvGuppZK9905Gjkwuu0xQ\nBOgIIRFgIarr5He/S7bZJtlpp+S115Jf/7osZf7wh5NFfFUGAOhUVZXssENyww3J9dcn/fsn++yT\nbLJJcumlyYwZrR4hQPfhR1aAhaCuk6uvTrbaKtlll+TNN5Mrr0zuvDPZa6/yBBcAgAVru+1KTLzh\nhmSFFZL99ks23ji55BJBEeDdEBIBFqC6Lpt7b7FFsuuu5c9XXZXcfnvZq0dABABY+D74wWT8+OQP\nf0hWXTXZf/9k2LDk4ovLnooAzJ2QCLAA1HWZcbj55iUY9ulTZiTecksJigIiAEDrbbVV+3O0Ndcs\nh7QMG1ZOfRYUAd5JSAToRHWdXHNNMmpU8qEPlVOX237bvcsuAiIAQFe0xRbtq0bWXjs54IASFH/x\nC0ueAWYlJAJ0khtvLKcwjx5dZiCOH1/239lxRwERAKA72GyzsqrkjjuSgQOTj3wkGTEiueIKpzwD\nJEIiwHt2223JzjuXiPjqq8lvfpPcfLOACADQXW26aZmheNNNyXLLlcPxRo0qK08ERaA3ExIBOuju\nu5M99yxLYZ55JvnlL5MJE5LddxcQAQB6gq23Lqc8jx+fLLJIWXmy7bZlJQpAbyQkAsyn++5L9tuv\nLHN58MGyGfe99yb77CMgAgD0NFVVVprccktZefLqqyUm7rxzWZkC0JsIiQDv0kMPtW+8PWFCcs45\nyf33l9P9+vRp9egAAFiQqqqsPJkwIbn00rIiZYstygF7d9/d6tEBLBxCIkCDp55KDj88WW+9cnjK\nmWcmkycnn/xk0rdvq0cHAMDCVFXJvvuWFSkXXVSeF44Ykey/f1mtAtCTCYkA8/CPfyTHHZcMHpz8\n6lfJN76R/OUvyZFHJost1urRAQDQSn36lJUp99+f/PjHZZnz+uuX54pPP93q0QEsGEIiwBymTk2+\n9rVk7bWT738/+dznkr/+NRk7NunXr9WjAwCgK+nbNznkkDIb8dRTk0suKb+I/u//Tl56qdWjA+hc\nQiLATG+9lfzoR8k66yRf+lJy4IHJww8nJ52ULLtsq0cHAEBX1q9fcuyx7b+A/ta3yi+mTzstmTat\n1aMD6BxCItDr1XXyi18kG2yQ/Nu/JdttlzzwQPLd7yYrr9zq0QEA0J3075985SvlF9If/Wjy+c8n\n666b/OQn5RfXAN2ZkAj0atddl2y+efKRj5TfGN99d3LhheV1AADoqFVXTX7wg2TSpGTLLZNDD02G\nDy97b9d1q0cH0DFCItAr3X13Mnp0suOO5eS9669Prroq2XjjVo8MAICeZJ11kp/9LLnrrmTAgHLi\n85ZbJjfe2OqRAcw/IRHoVZ54Ijn44GTkyOSxx5JLLy0n7G23XatHBgBATzZyZHLNNcn48WWJ87bb\nJh/+cDJ5cqtHBvDuCYlAr/Dyy+XkvHXXTX7727LM5M9/Lr8RrqpWjw4AgN5ixx2T229Pxo1L7rmn\n7NP9mc8kf/tbq0cG0ExIBHq06dOTM89MBg8uJ+d99rPJX/5SDlXp27fVowMAoDdaZJHkYx8rB/x9\n7Wtlj+7Bg5NTTnHCM9C1CYlAj1TXyeWXJ8OGJUcfneyxR/Lgg8nJJyfLLNPq0QEAQNKvX/lF98MP\nJ4cckpxwQjJkSHL++cmMGa0eHcA7CYlAj3PXXcn22yd7752ssUY5WOWcc8rm1gAA0NUsv3xZPXP/\n/cnmmyef+ESy6ablQECArkRIBHqMxx5LDjywPOl64YVyCvM11yQbbdTqkQEAQLPBg5NLLkluvjlZ\nbLFkhx2SD30omTSp1SMDKIREoNt7+eXk858vy0CuvTb53/8tG1fvuquDVAAA6H623DK55Zbk5z8v\nsxSHDUs+/WkHsgCtJyQC3dbbbydnn52ss07yne8kxx2XPPRQcthhDlIBAKB7q6rkIx8pIfEb30gu\nvrg87z399OTNN1s9OqC3EhKBbunGG8sS5sMPT3beuRykcuKJydJLt3pkAADQeRZfPBk7tvzC/MAD\nk//6r2TDDZMrrigHDAIsTEIi0K08+mj5zey22yaLLprcemtywQUOUgEAoGdbYYXk+99P7r03GTgw\n2WuvZPTo5L77Wj0yoDcREoFu4dVXky9+MRk6tOwXc955JSKOGtXqkQEAwMKz4YblQMHLL08eeaQc\nLHj00cmLL7Z6ZEBvICQCXdqMGclPf5qsu25yxhnJ5z6XTJ6cHHRQsoivYAAA9EJVVU5zvu++5NRT\nk/PPLyc+f/vbyfTprR4d0JP5MRzosm65pcw4/OQnk222SR54IDn5ZPsgAgBAkiy2WHLssWX/xP33\nL3spDh+eXHVVq0cG9FRCItDlPPlkcsAByVZblZOZb7wx+dnPyl4wAADA7FZaKfnhD5O7705WWSXZ\nffdymzy51SMDehohEegy3nijLM0YOjQZPz758Y+TO+4osxEBAIB/bqONkuuuSy69NJk0KRk2LPn8\n58t+4wCdQUgEuoSrry7LML74xeSII5IHH0wOOSTp06fVIwMAgO6jqpJ9903uvz85/viyb+LQocnF\nFyd13erRAd2dkAi01COPJPvsk+y6a7L66sm995ZDVfr3b/XIAACg+1piieRLXyozEzfbLBkzJtl+\n++TPf271yIDuTEgEWmLatOTEE5P110/uvLPsgXjttckGG7R6ZAAA0HMMGpT88pfJb3+bPPNMsvHG\nyX/+ZzJlSqtHBnRHQiKwUNV18utfl4D4la+Uk+Ue+P/t3XeYVdX9tvF70UVFNEaxJNaE2FABKYKA\nIM2KokTsYkMxCnZsRMXeS4hGLKBiRRFRAUW6UgREQcD8YktEQaMhdgT2+8caX9FQZoaZWXPOuT/X\nNZdw2DPzJJzNzDxnrfWdH6fMhZA6nSRJkpSfOnaEt96Ca66BgQPh97+HQYNgxYrUySTlEotESRXm\nnXfi9LguXeI5LXPmxG9kNtggdTJJkiQp/9WoARdcEKc5t2sHJ5wALVvCzJmpk0nKFRaJksrd119D\n376w665x9eGwYfDCC/FVUEmSJEkVa6utYMgQGDcOvvwSGjeG00+Hf/87dTJJlZ1FoqRyk2WxNNxp\nJ7jttjiR+e234ZBD3MYsSZIkpda6NcyaFb9XHzIk7hp64AG3O0taPYtESeXi/ffh4IPjRObddoO5\nc6Ffvzg9TpIkSVLlUK0anHVWPIaoUyfo0SMWjE53lrQqFomSytTSpXDttXGYyhtvwNChMGIEbL99\n6mSSJEmSVmfzzeGhh2DMGPj0U9hzz3ie4tdfp04mqTKxSJRUZsaPhz32gMsugzPOgHnz4LDD3MYs\nSZIk5Yq2bWH2bPjzn+HOO+MxRc8+mzqVpMrCIlHSOlu8GI4/Htq0gY03jlPfbrrJacySJElSLqpZ\nM55vPnduPKaoS5d4bNH776dOJik1i0RJpbZiBdxzD9SvH7cvDxwIEydCgwapk0mSJElaV9tvH7/P\nHzo0DmXZeWe47rp4nJGkwmSRKKlUZs2CvfeGnj3jQJUFC+Ckk6CK/6pIkiRJeSOEeFzR22/D6afD\npZfG8xPHj0+dTFIK/sgvqUS++gr69IHGjeOvJ06E+++HTTdNnUySJElSedlwQ7j55niM0UYbxWON\nTjgBPvssdTJJFckiUVKxPf887LJL3M587bVxVWLLlqlTSZIkSaooDRrApElw770wfHgcxvLww5Bl\nqZNJqggWiZLW6pNP4I9/hAMPjN8ozJ0LF1wA1aunTiZJkiSpolWpAiefDPPmQfv2cOyx0LEjvPtu\n6mSSyptFoqTVWrEivtK4004wdiw88gi8+CJst13qZJIkSZJS23xzGDIEXngB3nkHdt0VbrgBfvgh\ndTJJ5cUiUdIqzZ8P++4Lp54KXbrEVxuPOioetixJkiRJP+rcGebMicNY+vaFvfaC6dNTp5JUHiwS\nJf3M99/DlVfC7rvDwoUwZgw88AD86lepk0mSJEmqrDbYIA5jmTYtbn1u1iwOafzqq9TJJJUli0RJ\n/9+kSbDnnnDVVXDeefDmm9C2bepUkiRJknJFo0axTLz++jikcZdd4tBGSfnBIlES//lP3Iawzz5Q\npw7MmAFXXw3rrZc6mSRJkqRcU61aXJgwd248b/3AA+Pwxk8+SZ1M0rqySJQK3LBhsPPOcZDKnXfC\n5MnQoEHqVJIkSZJy3XbbxWGNjzwShzfutBPcfz9kWepkkkrLIlEqUIsXx1cFDz00bj+YOxfOPBOq\nVk2dTJIkSVK+CCEObZw3Dw4+GE46CTp2hPffT51MUmlYJEoFJstgyJC4CnHMmPjq4PDh8JvfpE4m\nSZIkKV/96lcwaBC88ALMnw+77gp33QUrVqROJqkkLBKlAvLRR/FVwKOPhv32g7ffjq8OhpA6mSRJ\nkqRC0LkzzJkDxx0Hf/oTtG4N77yTOpWk4rJIlApAlsHAgXEV4uuvwzPPwGOPwWabpU4mSZIkqdDU\nqQMDBsRzEz/+OJ7RfsMNsGxZ6mSS1sYiUcpz770H7dvDKadA165xFWKXLqlTSZIkSSp0bdrAm2/G\ns9r79oVmzeLvJVVeFolSnlq+HG6/PZ498ve/w8iRcULaxhunTiZJkiRJUe3acNNN8Oqr8O23cRBk\nv36wdGnqZJJWxSJRykPz50OrVtC7N5x4YjyDpGPH1KkkSZIkadWaNoWZM+PKxGuugYYNYdq01Kkk\n/ZJFopRHli+PZ4vssQd8+imMHx8noW24YepkkiRJkrRmNWvClVfGc91r1oTmzeHCC+G771Ink/Qj\ni0QpTyxYAC1bwkUXxTNGZs+OqxIlSZIkKZfsvjtMnQr9+8Ntt8XVidOnp04lCSwSpZy3fDncemtc\nhfjvf8OkSfGMkfXWS51MkiRJkkqnWrW4zXnGjPizTfPmcOml8P33qZNJhc0iUcph//d/cdLZuedC\nz57wxhuw996pU0mSJElS2dh1V5gyJQ5guf562GsvmDUrdSqpcFkkSjloxQq4805o0AAWLoRx4+Kq\nxNq1UyeTJEmSpLJVvTpcdlnc3lylCjRpAldcAT/8kDqZVHgsEqUc89570K4dnHUW9OjhWYiSJEmS\nCsMee8RJzn37wlVXxUnPb72VOpVUWCwSpRyRZXDPPXEV4nvvwZgxcSLzBhukTiZJkiRJFaNGjTjZ\neepUWLoUGjWCa66BZctSJ5MKg0WilAM+/BA6dIjnIB51FLz5JrRtmzqVJEmSJKXRqFEcxHLeeXHb\n8957w7x5qVNJ+c8iUarEsgweeAB22w3mz4eRI+OqxDp1UieTJEmSpLRq1oyrEV99Fb78EvbcE266\nCZYvT51Myl8WiVIltXgxHHpoPAfx0EPj2R8dO6ZOJUmSJEmVS9OmMHMmnHkmXHBB3L31/vupU0n5\nySJRqoSGD4+rECdPhmeegQcfhLp1U6eSJEmSpMppvfXiasSxY+GDD+LZ8g8+GHd5SSo7FolSJfLl\nl3DyyXDIIfFVtTlzoEuX1KkkSZIkKTe0bh3PlO/aFU48Mf73009Tp5Lyh0WiVElMmgS77w6PPQb3\n3gvPPgubb546lSRJkiTlljp14lnzTz8NEyfCrrvCiBGpU0n5wSJRSmzpUujbF1q1gi22gNmz46rE\nEFInkyRJkqTc9eNZ802awEEHwamnwldfpU4l5TaLRCmhOXPiF7Wbb47TxiZMgB12SJ1KkiRJkvJD\nvXrxDPq//Q2GDIm7wCZPTp1Kyl0WiVICK1bE8rBRI1i2DKZNg4sugqpVUyeTJEmSpPwSApxyStz9\nVa9e3A128cVxd5ikkrFIlCrYBx9A27Zw/vlw5pnw+uuwxx6pU0mSJElSftthh7gLrH9/uPHGnwZc\nSio+i0SpAg0ZAg0awLvvwpgxcVVirVqpU0mSJElSYahaNZ5RP21aXJHYuDHccQdkWepkUm6wSJQq\nwJIlcMwxcPTRcOCB8OabsO++qVNJkiRJUmHac0+YMQNOOw3OPhv23x8WLUqdSqr8LBKlcjZ5cty6\nPHw4PPwwPPII1K2bOpUkSZIkFbZateD22+GFF2DWLNhtN3j++dSppMrNIlEqJ8uWwZ//HA/y3XLL\neLDv0UenTiVJkiRJWlnnznHXWJMmcQfZmWfCt9+mTiVVThaJUjl4991YIPbvD/36wfjxsN12qVNJ\nkiRJklZls83guefgrrvgvvvi2YmzZ6dOJVU+FolSGcoyeOihuJX5k09g4kS4/HKoVi11MkmSJEnS\nmoQAvXrB66/Hn+GaNIFbb4UVK1InkyoPi0SpjPznP3DUUXDccdClC7zxBjRvnjqVJEmSJKkkdtkF\npk6NpeI558Stzx9/nDqVVDlYJEplYOJE2H13ePFFGDIEBg+GOnVSp5IkSZIklUatWnDLLTBqVDw/\ncbfd4gBNqdBZJErr4Icf4LLLoE0b2GabeIZG9+6pU0mSJEmSykKHDrFIbNECDjkETj8dvvkmdSop\nHYtEqZQ++ABat4Zrr4Urr4SxY2OZKEmSJEnKH7/+NQwbBnffDYMGwV57wZw5qVNJaVgkSqUwdGgc\nqLJwYdzWfMklULVq6lSSJEmSpPIQApx2WhzEUqVKLBPvuScO3JQKiUWiVALffhuXsh9+OOy3nwNV\nJEmSJKmQ7LwzTJsGJ5wAPXvCEUfEwZtSobBIlIpp7lxo0gQefDC+8vTEE1C3bupUkiRJkqSKtN56\n8Ne/wlNPwZgxcbfaa6+lTiVVDItEaS2yDAYOjEvXswymT4dTT41L2yVJkiRJhalr17hLbautYJ99\n4vn5K1akTiWVL4tEaQ2WLIEjj4RTToFjj41L2HfdNXUqSZIkSVJlsM02MH48XHRRPDu/Qwf4+OPU\nqaTyY5EorcbUqXGJ+qhRcRvzPfdA7dqpU0mSJEmSKpNq1aB/f3jppXgk1u67w8iRqVNJ5cMiUfqF\nFSvg+uuhZUuoVy8uVT/iiNSpJEmSJEmVWbt2MHs2NG4MnTvD+efD0qWpU0llyyJRWsmiRdCpE/Tt\nC+edBxMmwLbbpk4lSZIkScoFm20GI0bATTfB7bfHBSrvvps6lVR2LBKlIuPGxa3Mb74ZtzNfey1U\nr546lSRJkiQpl1SpAueeC6++Cp9/DnvuCUOHpk4llQ2LRBW85cvjeRbt2sHOO8etzO3bp04lSZIk\nScpljRvDjBnQsSMcfjicfTZ8/33qVNK6sUhUQVu8OJ5dcfnlcNllMHp0PBdRkiRJkqR1tdFG8Pjj\ncNddcPfdcavze++lTiWVnkWiCtb48XEr8+zZsUD885+hatXUqSRJkiRJ+SQE6NUrbnX+97/jVudn\nnkmdSiodi0QVnBUr4JproG1bqF8/bmXeb7/UqSRJkiRJ+axRI5g5Mx6rddhh0KePU52VeywSVVA+\n/RT23x8uvRQuuQRefhm22CJ1KkmSJElSIahbF556Kk50/stfYJ994P33U6eSis8iUQVj4sS4lXnm\nTBg5Eq680q3MkiRJkqSKFQKcdRZMmhTP7d9zT3j22dSppOKxSFTeW7ECrrsO9t0XdtwxbmXu0CF1\nKkmSJElSIWvSJC50adMGunSBc891q7MqP4tE5bXPPoMDD4S+feGii2DMGNhyy9SpJEmSJEmCjTeG\np5+GW2+FO+6AVq3ggw9Sp5JWzyJReWvqVGjYEKZPj1uZ+/eHatVSp5IkSZIk6SchQO/ecavzxx/H\nn2NHjkydSlq1UhWJIYReIYT3QgjfhhCmhBD2WsO1J4cQJoQQPi96e2lN10vrKst+OrR2661h1izo\n2DF1KkmSJEmSVq9p0/jza7NmcUhov36wfHnqVNLPlbhIDCH8EbgZ6AfsCcwGRoUQNl3Nu7QGhgBt\ngGbAP4HRIQRn5arMffUVHH00nHkmnH46jBsXy0RJkiRJkiq7TTaB556Lw0GvuioWip99ljqV9JPS\nrEjsA9yTZdngLMvmAz2Bb4Aeq7o4y7Jjsyy7O8uyN7Msewc4uejztittaGlV5s+Pr+AMHw6PPQa3\n3w41aqROJUmSJElS8VWpApdeCqNGxWEsDRvCtGmpU0lRiYrEEEJ1oBEw5sfHsizLgJeB5sX8MOsD\n1YHPS/K5pTV58knYa6+4rXn6dPjjH1MnkiRJkiSp9Nq3j0XiVltBy5YwYED8mVdKqaQrEjcFqgKL\nfvH4IqBeMT/G9cBHxPJRWic//AB9+kC3bnE687RpsNNOqVNJkiRJkrTufvMbGD8eevaEXr3g2GPh\n669Tp1IhK6sZtgFYay8eQrgI6Aa0zrJs6dqu79OnDxtttNHPHuvevTvdu3cvbU7lkY8+iisPp06F\nOwTjBu0AAB1zSURBVO6I5yKGkDqVJEmSJEllp0aN+DNv8+Zw8snwxhswdCjUr586mSqTRx99lEcf\nffRnjy1ZsqTMP0/ISrAutmhr8zdA1yzLhq/0+IPARlmWHbqG9z0PuBhol2XZrLV8nobAjBkzZtCw\nYcNi51PheOUV6N4dqleP25qbF3djvSRJkiRJOertt6Fr17iw5v774fDDUydSZTZz5kwaNWoE0CjL\nspll8TFLtLU5y7IfgBmsNCglhBCKfv/q6t4vhHA+cAnQcW0lorQmK1bAddfFsyIaNIBZsywRJUmS\nJEmFYeed45FenTvDEUfAuefGI7+kilKaqc23AKeGEI4LIfwBuBuoDTwIEEIYHEK45seLQwgXAFcR\npzp/GELYvOht/XVOr4KyZAkceij07QsXXwwjR8Kvf506lSRJkiRJFWfDDeGxx+D22+OW57Zt4eOP\nU6dSoShxkZhl2RPAucCVwCygAXGl4adFl2zNzwevnE6c0vwUsHClt3NLH1uFZu7cOJV5wgR47jm4\n6iqoWjV1KkmSJEmSKl4IcNZZcRDLu+9Co0bw6mr3iUplpzQrEsmybECWZdtmWbZelmXNsyx7faU/\na5tlWY+Vfr9dlmVVV/F2ZVn8D1D+e/JJaNoUatWC6dPjdGZJkiRJkgrd3nvDjBmwww7Qpg0MGAAl\nGIUhlVipikSpIixbBhdeCN26wUEHwWuvwY47pk4lSZIkSVLlUa8ejBkDPXtCr17Qowd8913qVMpX\nFomqlD77LB4ee/PN8W3IEFjfUzUlSZIkSfofNWrE8xIHD47nJ7ZsCR9+mDqV8pFFoiqdmTOhcWN4\n4w146SU455x4/oMkSZIkSVq9Y4+FyZPj4pxGjeCVV1InUr6xSFSlMngwtGgRpzHPmAH77ps6kSRJ\nkiRJuaNhw/jz9J57Qvv2cNNNnpuosmORqEph6VI480w4/njo3h0mToTf/jZ1KkmSJEmScs+vfgUv\nvggXXADnnx9/zv7669SplA8sEpXcJ59Au3bwt7/BX/8K990XJzRLkiRJkqTSqVoVrr0WnnoKRoyA\nZs3g//4vdSrlOotEJfXaa3HZ9T/+AePHxylTnocoSZIkSVLZ6NoVpk2LOwEbN4bnn0+dSLnMIlHJ\n/O1v0Lo1bL99PL+hefPUiSRJkiRJyj877xzLxNat4aCDoH9/z01U6VgkqsItXQpnnAGnnQannBKn\nSG2xRepUkiRJkiTlr402gmeegX794LLLoFs3+Oqr1KmUaywSVaEWL45TowYOjCsS//IXqFEjdSpJ\nkiRJkvJflSqxSHzmGRg5Elq0gPfeS51KucQiURVm1izYay9YsADGjo2rESVJkiRJUsXq0iXOLPjq\nq/hz+tixqRMpV1gkqkI89lh8pePXv4bp0+OvJUmSJElSGrvuGn8+b9gw7hy8807PTdTaWSSqXC1f\nDn37QvfucVLUxInwm9+kTiVJkiRJkjbZBF54Ac4+G846C04+Gb7/PnUqVWYWiSo3S5bAwQfDDTfA\nTTfB4MGw3nqpU0mSJEmSpB9VqwY33wyDBsEjj0CbNvDxx6lTqbKySFS5WLAAmjaFV1+Nr26cey6E\nkDqVJEmSJElaleOOgwkT4MMPoXFjmDYtdSJVRhaJKnMvvABNmsRpUNOnQ8eOqRNJkiRJkqS1adIE\nXn8dttkGWrWKOwullVkkqsxkGVx3HRx4YFwKPWUK7Lhj6lSSJEmSJKm4ttgiTnE+5hg4/ng45xxY\ntix1KlUWFokqE99+C0cfHQerXHopPPMM1KmTOpUkSZIkSSqpmjXh3nvjJOc77oD994cvvkidSpWB\nRaLW2cKF0Lo1DBsGTz4JV14ZtzVLkiRJkqTcFAKceSaMHh23OzdrBu+8kzqVUrPu0TqZMSOeobBw\nIUyaBIcfnjqRJEmSJEkqK23bxsErVarEoaovv5w6kVKySFSpPfkk7LMPbLllHKrSsGHqRJIkSZIk\nqaztuCO89losEjt1ggEDUidSKhaJKrEsgyuugG7doEsXGD8+HsYqSZIkSZLyU926MGJE3O7cqxec\ncQb88EPqVKpo1VIHUG755hs48UR44gno3x8uvjiemyBJkiRJkvJbtWpw222wyy6xSHznndgPbLJJ\n6mSqKK5IVLF99FEcqjJiBAwdCpdcYokoSZIkSVKhOeWUeFbiG2/E7c7z56dOpIpikahimT4d9toL\nFi2CyZPhsMNSJ5IkSZIkSam0bh2HsNSoESc6jx6dOpEqgkWi1urxx6FVK9hmm/iPxB57pE4kSZIk\nSZJS2377OISlRQvo3BnuuCPOVVD+skjUaq1YAf36wZFHwuGHw9ixUK9e6lSSJEmSJKmyqFMHhg+H\n3r3h7LOhZ0+HsOQzh61olb75Bk44AZ56Cq69Fi680PMQJUmSJEnS/6paFW6+GXbeGU4/PQ5hGTrU\nISz5yBWJ+h+ffAJt2sDzz8PTT8NFF1kiSpIkSZKkNTvpJBgzBt56C5o3h7//PXUilTWLRP3MW2/F\niUsffQQTJ0KXLqkTSZIkSZKkXLHPPjBlSvx1s2YwYULaPCpbFon6/0aOjAekbrIJTJ0KDRumTiRJ\nkiRJknLNjjvGMnH33WG//WDw4NSJVFYsEgXAX/4CBxwQtzRPnAhbb506kSRJkiRJylUbbxwXLB17\nLBx/PFx2WRzqqtzmsJUCt3w5nHNOHNHepw/ceGM8JFWSJEmSJGld1KgBAwfC738f5y/8/e/wwAOw\n3nqpk6m0XJFYwL78Eg45JK5GHDAAbrnFElGSJEmSJJWdEODCC+Gpp2D4cGjbFhYtSp1KpWWRWKD+\n+U9o2TJuY37++TieXZIkSZIkqTx07Qrjx8P778chLHPnpk6k0rBILECvvw5NmsCSJTB5MnTsmDqR\nJEmSJEnKd3vtFYe7brgh7L03jB6dOpFKyiKxwDz9NLRqBdtsE2/eXXdNnUiSJEmSJBWK3/4WJk2C\nFi1g//3h7rtTJ1JJWCQWiCyDG26IS4kPOgjGjoXNN0+dSpIkSZIkFZo6deJ5iWecEY9aO+ecOAxW\nlZ9TmwvAsmXQqxf87W9w8cVw1VVQxQpZkiRJkiQlUq0a3HEH/O530Ls3vPcePPII1K6dOpnWxDop\nz335JRx8MNx/P9x3H1x9tSWiJEmSJEmqHP70Jxg2LJ6X2LYtLF6cOpHWxEopjy1cCK1bx7MHnn8e\nevRInUiSJEmSJOnnDjrop4nOzZvDggWpE2l1LBLz1Jw5cZz64sWxSOzQIXUiSZIkSZKkVWvcGKZM\ngZo140TniRNTJ9KqWCTmoTFj4vSjTTaJk5kbNEidSJIkSZIkac223RYmT449xn77weOPp06kX7JI\nzDODBkGnTnEp8IQJsNVWqRNJkiRJkiQVz8Ybw8iR0K0bHHkkXH89ZFnqVPqRU5vzRJbBFVfEt5NP\nhgEDoHr11KkkSZIkSZJKpmZNGDw4rlC86KI40fmuu+KkZ6XlX0EeWLoUTj01rka8+mro2xdCSJ1K\nkiRJkiSpdEKAq66C7baD006Df/4zbnXeYIPUyQqbW5tz3JIlsP/+8Oij8MgjcPHFloiSJEmSJCk/\n9OgBzz8fh6+0agULF6ZOVNgsEnPYhx/GoSozZsDo0XDUUakTSZIkSZIkla0OHWDSJFi8GJo1gzlz\nUicqXBaJOeqNN+LN8/XX8Oqr0Lp16kSSJEmSJEnlo0EDmDIlDmNp0QJeeSV1osJkkZiDXn45Lufd\nckt47TXYaafUiSRJkiRJksrX1lvHLc7NmkGnTvGYN1Usi8Qc8/DD0LkztGwJ48ZBvXqpE0mSJEmS\nJFWMOnVgxAjo3j0e8XbzzakTFRanNueILIMbb4QLL4QTT4R77oHq1VOnkiRJkiRJqljVq8ODD8YV\niuedB//6VywUq7hcrtxZJOaA5cuhTx+480647DK44gonM0uSJEmSpMIVAlx9dTz27U9/itOcBw2C\nWrVSJ8tvFomV3HffwTHHwDPPwN13w2mnpU4kSZIkSZJUOfTqFcvEo46K5yYOGwZ166ZOlb9c9FmJ\nffFFHHH+wguxSLRElCRJkiRJ+rlDD42Dad98E/bZJ251VvmwSKykPvwwDlR5+20YMwYOPjh1IkmS\nJEmSpMqpRQuYPBn++19o3hzmzk2dKD9ZJFZCb74Zn/TffBNvgubNUyeSJEmSJEmq3HbaCV57DTbZ\nJC7OmjAhdaL8Y5FYyYwdG5fhbr55fPLXr586kSRJkiRJUm7YcstYIDZsCO3bw1NPpU6UXywSK5HH\nH48HgzZtCuPHQ716qRNJkiRJkiTllo02ghdfhK5doVs3uPPO1Inyh1ObK4lbb4VzzokTmu+7D2rU\nSJ1IkiRJkiQpN9WoAQ8/DFttBWedFQewXHcdhJA6WW6zSEwsy+Cii+CGG+DCC+Haa31SS5IkSZIk\nrasqVeDGG2OZ2KcPLFoE994L1aunTpa7LBITWrYMTj0VHnggrkjs3Tt1IkmSJEmSpPzSuzdsthkc\nfzx89hk88QTUrp06VW7yjMREvvkGDjsMHnooLrW1RJQkSZIkSSofRx0FI0bAuHFxCMvnn6dOlJss\nEhP44gvo0AHGjIHnnoOjj06dSJIkSZIkKb917AivvAILFkCrVvHcRJWMRWIF++ij+GSdNy8+eTt1\nSp1IkiRJkiSpMDRpApMmwZdfQosWMH9+6kS5xSKxAi1YEJ+kS5bEJ23TpqkTSZIkSZIkFZY//AEm\nT4YNN4SWLWHatNSJcodFYgWZPj0+OddfH159FXbaKXUiSZIkSZKkwrT11jBhAtSvD23bwujRqRPl\nBovECvDSS7DvvvC738HEifHJKkmSJEmSpHQ22eSnzuaAA2DIkNSJKj+LxHL26KPxydi6Nbz8cnyS\nSpIkSZIkKb3ateHpp+Mg3KOPhttvT52ocquWOkA+u+MOOPtsOO44GDgQqldPnUiSJEmSJEkrq14d\nHngANt8ceveGRYvg6qshhNTJKh+LxHKQZXD55dC/P5x3Hlx/PVRx7ackSZIkSVKlFELsbzbbLHY5\nixfD3XdDNZuzn/H/jjK2YgWceSb89a9www1w/vmpE0mSJEmSJKk4zj03loknnghffBHPTaxZM3Wq\nysMisQz98AMcfzw8/jjcdx/06JE6kSRJkiRJkkri2GOhbl3o1g0OPBCeeQY22CB1qsrBDbdl5Jtv\noEsXGDoUnnjCElGSJEmSJClXHXQQjBwJU6fCfvvB55+nTlQ5WCSWgSVLoFMnGD8eRoyArl1TJ5Ik\nSZIkSdK6aN0axo6Ff/wj/nrhwtSJ0rNIXEeLF0ObNjBnDrz8MrRvnzqRJEmSJEmSykKjRjBxIvzn\nP9CyZSwVC5lF4jr48EPYZx/45JO4GrFZs9SJJEmSJEmSVJb+8AeYNClOcG7ZEt56K3WidCwSS2n+\nfGjRIg5YmTwZdtstdSJJkiRJkiSVh222iWVivXpxm/OUKakTpWGRWAozZsSViHXrxifR9tunTiRJ\nkiRJkqTytNlmMG4c7LILtGsHL72UOlHFs0gsoXHjYN99YYcd4nbmLbdMnUiSJEmSJEkVYaONYNSo\nuCrxgANg6NDUiSqWRWIJPPdcnM7ctGkcrLLJJqkTSZIkSZIkqSLVrg3DhkHXrtCtG9x3X+pEFada\n6gC54uGH4YQT4JBDYMgQqFkzdSJJkiRJkiSlUKNG7Irq1oWTT4YvvoDzzkudqvxZJBbDgAHQqxf0\n6AH33BOn9EiSJEmSJKlwVa0aO6Nf/QrOPx/+8x+46ioIIXWy8mMlthY33AAXXgi9e8Mtt+T3k0GS\nJEmSJEnFFwL07x9XJp5/Pnz1Fdx6a/72RxaJq5Fl0K9fbJIvvxz+/Of8fRJIkiRJkiSp9M47D9Zf\nH844I5aJ99wTVyzmG4vEVcgyOOccuO22uCLx/PNTJ5IkSZIkSVJldvrpsUw88UT4+msYPBiqV0+d\nqmxZJP7C8uXQsycMHBj3uZ9+eupEkiRJkiRJygXHHRfLxO7dY5n4xBNQq1bqVGWnSuoAlckPP8Cx\nx8L998OgQZaIkiRJkiRJKpmuXWH4cHjpJTjwwFgo5guLxCLffQdHHAFPPRXb4uOOS51IkiRJkiRJ\nuahTJxg5EqZOhQ4d4kTnfGCRSGyGDzoIRo2CZ5+NzbEkSZIkSZJUWq1bw8svw7x50LYtfPZZ6kTr\nruCLxCVLoGNHmDIlNsWdO6dOJEmSJEmSpHzQtCmMGwcffRSLxYULUydaNwVdJH72WWyE3347NsSt\nW6dOJEmSJEmSpHzSoAFMmAD//S/ssw+8/37qRKVXsEXixx/H4vBf/4rNcNOmqRNJkiRJkiQpH9Wv\nDxMnQgixTFywIHWi0inIIvGDD+Jf2n//GxvhBg1SJ5IkSZIkSVI+23bb2EPVqQOtWsGbb6ZOVHIF\nVyT+4x/xLyvLYhNcv37qRJIkSZIkSSoEW24J48fD1lvDvvvCjBmpE5VMQRWJCxbEErFWrdgAb7tt\n6kSSJEmSJEkqJJtuCmPGwO9+B+3axQHAuaJgisQ5c+KZiBtvHJvfrbZKnUiSJEmSJEmFqG5dGD0a\ndtsN2rePu2ZzQUEUibNmQZs2sMUWcbBKvXqpE0mSJEmSJKmQ1akDI0dCkybQqVNcpVjZ5X2ROH06\ntG0L228f/0I23TR1IkmSJEmSJAnWXx9GjIhH8R1wALz4YupEa5bXReLkyXGv+c47w0svwSabpE4k\nSZIkSZIk/WS99WDYMOjQAbp0gWefTZ1o9fK2SBw3Djp2hIYNYdQo2Gij1IkkSZIkSZKk/1WzJjz1\nFBx8MBx+ODz5ZOpEq5aXReLo0dC5M+y9N7zwAmywQepEkiRJkiRJ0urVqAGPPgrdusGRR8LDD6dO\n9L+qpQ5Q1kaMgK5dYb/9YOhQqFUrdSJJkiRJkiRp7apVg8GD4wrF446DpUuhR4/UqX6SV0Xi00/H\nxvbAA+Gxx2KTK0mSJEmSJOWKqlVh4MBYJp50Enz/PZx+eupUUd4UiY89BsccE/eRP/QQVK+eOpEk\nSZIkSZJUclWqwIABsUw844xYJvbunTpVnhSJgwbFZZ5HHw0PPBCbW0mSJEmSJClXhQC33hrLxD59\nYpl44YVpM+X8sJUsi+ci9uhhiShJkiRJkqT8EQJcdx1cfnnsv374IW2enF+RGAIMGRILxCo5X4tK\nkiRJkiRJPwkBrrgCLrkk/VF+OV8kQvr/EyVJkiRJkqTyVBmGCruGT5IkSZIkSdJaWSRKkiRJkiRJ\nWiuLREmSJEmSJElrZZEoqcI9+uijqSNIKife31L+8v6W8pf3t6TiKlWRGELoFUJ4L4TwbQhhSghh\nr7Vcf0QIYV7R9bNDCJ1LF1dSPvAbFSl/eX9L+cv7W8pf3t+SiqvERWII4Y/AzUA/YE9gNjAqhLDp\naq5vDgwB7gX2AIYBw0IIO5c2tCRJkiRJkqSKVZoViX2Ae7IsG5xl2XygJ/AN0GM1158NvJhl2S1Z\nli3IsqwfMBM4s1SJJUmSJEmSJFW4EhWJIYTqQCNgzI+PZVmWAS8DzVfzbs2L/nxlo9ZwvSRJkiRJ\nkqRKploJr98UqAos+sXji4D6q3mfequ5vt4aPk8tgHnz5pUwnqRcsGTJEmbOnJk6hqRy4P0t5S/v\nbyl/eX9L+WmlXq1WWX3MkhaJqxOArAyv3xbgmGOOWYdIkiqzRo0apY4gqZx4f0v5y/tbyl/e31Je\n2xZ4tSw+UEmLxM+A5cDmv3h8M/531eGPPinh9RC3Ph8NvA98V8KMkiRJkiRJUqGrRSwRR5XVBwzx\niMMSvEMIU4CpWZadXfT7AHwI3JFl2Y2ruP4xYL0syw5Z6bHJwOwsy85Yl/CSJEmSJEmSKkZptjbf\nAgwKIcwAphGnONcGHgQIIQwG/pVl2cVF198OjA8hnAM8D3QnDmw5Zd2iS5IkSZIkSaooJS4Ssyx7\nIoSwKXAlccvyG0DHLMs+Lbpka2DZSte/FkLoDlxd9PZ34JAsy95e1/CSJEmSJEmSKkaJtzZLkiRJ\nkiRJKjxVUgeQJEmSJEmSVPlZJEqSJEmSJElaqyRFYgihVwjhvRDCtyGEKSGEvdZy/REhhHlF188O\nIXSuqKySSqYk93cI4eQQwoQQwudFby+t7d8DSemU9Ov3Su93ZAhhRQjh6fLOKKl0SvH9+UYhhL+E\nEBYWvc/8EEKnisorqfhKcX/3LrqnvwkhfBhCuCWEULOi8koqnhDCPiGE4SGEj4q+1z64GO/TJoQw\nI4TwXQjhnRDC8SX9vBVeJIYQ/gjcDPQD9gRmA6OKBris6vrmwBDgXmAPYBgwLISwc8UkllRcJb2/\ngdbE+7sN0Az4JzA6hLBF+aeVVBKluL9/fL9tgBuBCeUeUlKplOL78+rAy8BvgcOA+sApwEcVElhS\nsZXi/j4KuLbo+j8APYA/EgenSqpc1icOQO4FrHUASghhW2AEMAbYHbgdGBhCaF+ST1rhw1ZCCFOA\nqVmWnV30+0AsD+7IsuyGVVz/GFA7y7KDV3rsNWBWlmVnVFBsScVQ0vt7Fe9fBfgC6JVl2cPlGlZS\niZTm/i66p8cD9wOtgI2yLDusgiJLKqZSfH/eEzgX+EOWZcsrNKykEinF/X0n8d5uv9JjNwFNsixr\nVUGxJZVQCGEF0CXLsuFruOZ6oHOWZQ1WeuxR4vfo+xf3c1XoisSiVy8bEdtPALLYZL4MNF/NuzUv\n+vOVjVrD9ZISKOX9/UvrA9WBz8s8oKRSW4f7ux+wOMuyB8o3oaTSKuX9fRDwGjAghPBJCOGtEELf\nohcPJFUSpby/XwUa/bj9OYSwPbA/8Hz5ppVUAZpRBv1atTKLUzybAlWBRb94fBFxS8Sq1FvN9fXK\nNpqkdVSa+/uXridui/rlP26S0irx/R1CaAGcSNw2IanyKs3X7+2BtsDDQGfgd8CAoo/Tv3xiSiqF\nEt/fWZY9WrTteVLR6sWqwN1Zll1frkklVYTV9Wt1Qgg1syz7vjgfpKKLxNUJFGM/9zpcLymdYt2v\nIYSLgG5A6yzLlpZ7KkllYZX3dwhhA+Ah4JQsy76o8FSSysKavn5XIf7gcWrR6qZZIYStgPOwSJRy\nwWrv7xBCG+BioCcwDdgRuCOE8HGWZd7fUv4JRf8tdsdW0UXiZ8ByYPNfPL4Z/9uK/uiTEl4vKY3S\n3N8AhBDOAy4A2mVZNrd84klaByW9v3cAtgGeK1rNAEXHqYQQlgL1syx7r5yySiqZ0nz9/hhYmv38\nsPV5QL0QQrUsy5aVfUxJpVCa+/tKYPBKx5LMLXqB8B58oUDKdavr1/5bksU8FXqOSZZlPwAzgHY/\nPlb0A0Y74lkMq/LaytcXaV/0uKRKopT3NyGE84FLgI5Zls0q75ySSq4U9/c8YDdgD+LW5t2B4cAr\nRb/+ZzlHllRMpfz6PZm4Smll9YGPLRGlyqOU93dtYMUvHltR9K5hFddLyh2r6tc6UMJ+LcXW5luA\nQSGEGcSl0n2I/1g9CBBCGAz8K8uyi4uuvx0YH0I4h3jAa3figbGnVHBuSWtXovs7hHAB8VXP7sCH\nIYQfXx35Ksuyrys4u6Q1K/b9XfSK5tsrv3MI4T/EM97nVWhqScVR0u/P/wqcGUK4HbgL+D3QF7it\ngnNLWruS3t/PAX1CCG8AU4lnoF4JPPuLVciSEgshrE98Ye/Hkn/7EMLuwOdZlv0zhHAtsGWWZccX\n/fndxK/f1wP3E0vFw4kDlYqtwovELMueKDq89Urikso3iCuRPi26ZGtg2UrXvxZC6A5cXfT2d+CQ\nLMveRlKlUtL7GzidOKX5qV98qCuKPoakSqIU97ekHFGK78//FULoANwKzCYOSrsVuKFCg0taq1J8\n/b6KuALxKmAr4FPiroJLKyy0pOJqDIwlnm+YATcXPT4I6EEcrvKbHy/Osuz9EMIBxBcYzgL+BZyU\nZVmJhp0GX1SQJEmSJEmStDYVekaiJEmSJEmSpNxkkShJkiRJkiRprSwSJUmSJEmSJK2VRaIkSZIk\nSZKktbJIlCRJkiRJkrRWFomSJEmSJEmS1soiUZIkSZIkSdJaWSRKkiRJkiRJWiuLREmSJEmSJElr\nZZEoSZIkSZIkaa0sEiVJkiRJkiSt1f8DkHqqx548iB0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112618c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "p = np.linspace(0.01,0.99,100)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(p,1-p**2-(1-p)**2)\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that if $  p_H = 0  $ or $  p_H = 1  $ there is no misclassification. If $  p_H = 0.5  $, we see that we get the coin flip wrong 50% of the time. This should match our intuition.\n",
    "\n",
    "#### Entropy\n",
    "\n",
    "Another, and less intuitive, measure of prediction accuracy is entropy. It is defined as the expected information content of a class. If $f_c$ is the fraction of the class in a dataset, and $  I(c)  $ is the information content in the class, then entropy is defined as following\n",
    "$$  Entropy = \\sum_c f_c * I(c)  $$\n",
    "For reasons I will not get into in this post, $  I(c) = - log_2\\left(f_c\\right)  $, so the entropy of a dataset follows:\n",
    "$$  Entropy = - \\sum_c f_c * log_2\\left(f_c\\right)  $$\n",
    "The base 2 of the log can be anything, really. But since we will be using binary trees throughout this series, I will keep the base 2.\n",
    "\n",
    "#### Entropy for a Coin Flip\n",
    "\n",
    "Continuing with the coinflip example, I have produced the entropy of a coinflip using different values of $  p_H  $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABRIAAAKaCAYAAABC5aWJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112f2d610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(p,-p*np.log2(p)-(1-p)*np.log2(1-p))\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected information content of $  p_H = 0  $ and $  p_H = 1  $ is zero, which means that we always know what the next flip is. The entropy is maximum for $  p_H = 0.5  $, which is where we have the highest misclassification rate\n",
    "\n",
    "### Regressions Metrics for Decision Trees\n",
    "\n",
    "The metric used for decision tree regression is a reduction of  variance around the prediction.\n",
    "\n",
    "$$  Variance = \\frac{1}{N} \\sum_{i}^{N} ( y_i - \\bar{y} )^2  $$\n",
    "\n",
    "#### Simple Linear Example\n",
    "\n",
    "We can take a simple linear example and split it into two regions. The solid black line is the average of the entire dataset, while the red lines are the predictions in each region.\n",
    "\n",
    "<img class=\"aligncenter size-full wp-image-2747\" src=\"https://bryantravissmithdotcom.files.wordpress.com/2016/12/output_11_0.png\" alt=\"output_11_0\" width=\"768\" height=\"384\" />\n",
    "\n",
    "The total variance before the split was 40.00, while the weighted variance after the split is $  \\frac{6}{11} 11.67 + \\frac{5}{11} 8.00  $ = 10.0\n",
    "\n",
    "## Improving Metrics - Splitting on Information Gain\n",
    "\n",
    "Now that we have metrics to evaluate the prediction from a data set, we can now define the improvement metric to optimize. I will be using the term information gain, which it is the correct term when talking about entropy, for all the optimizations. I can not say conclusively is the name for the measure in decision tree regression. Being wrong hasn't stopped me before!\n",
    "\n",
    "### Decision Tree Classifier \n",
    "\n",
    "$$  Information \\ Gain = Gini \\ Impurity(d) - \\sum_{split \\ s} \\frac{|s|}{|d|} Gini \\ Impurity(s)  $$\n",
    "\n",
    "$$  Information \\ Gain = Entropy(d) - \\sum_{split \\ s} \\frac{|s|}{|d|} Entropy(s)  $$\n",
    "\n",
    "### Decision Tree Regressor\n",
    " \n",
    "\n",
    "$$  Information \\ Gain =  Variance(d) - \\sum_{split \\ s} \\frac{|s|}{|d|} Variance(s)  $$\n",
    "\n",
    "## Parameters of Decision Trees\n",
    "\n",
    "Decisions Trees are prone to overfit because the metrics can be driven to the minimum by making the tree deep enough capture each data point individually. The variance of a single point is zero. The misclassification rate of a single point is also zero. Traditionally this overfit is managed by pruning the tree after it is constructed, or setting parameters the limit how finely the data can be partitioned into regions.\n",
    "\n",
    "The post-pruning of a decision tree uses some sort of statistical evaluation on a validation set. If the error on the sub-nodes is larger than the node, the sub-nodes are removed.\n",
    "\n",
    "The parameters that are commonly used in CART to limit the partitioning of the data:\n",
    "\n",
    "1. Maximum Depth of the Tree\n",
    "2. Minimum Number of Data Points for a New Node\n",
    "3. Minimum Improvement needed for a New Node\n",
    "4. Minimum Metric under which New Nodes are not created.\n",
    "\n",
    "## Pseudo Code\n",
    "Now that we have a metric and measure of improvement, we can outline the pseudo code we will be implementing in the next posts which are based on the CART algorithm for decision trees. Classification and Regression Trees (CART) algorithm that recursively and greedily splits nodes if there is an improve in the metric of choice until some stopping conditions are met.\n",
    "\n",
    "<ol>\n",
    "\t<li>Set Training Parameters</li>\n",
    "\t<li>Create a Root Node</li>\n",
    "\t<li>Determine optimal split the node such that that split minimizes the metrics of concern.</li>\n",
    "\t<li>If stopping parameters are not violated, create two child nodes and apply step 3 and 4 to each child node.</li>\n",
    "</ol>\n",
    "\n",
    "# Code\n",
    "\n",
    "In this code section. I am going to be implementing a Decision Tree algorithm, but I will do it in the following parts:\n",
    "\n",
    "<ol>\n",
    "\t<li>Introduce the Data</li>\n",
    "\t<li>Script Finding the Best First Partition</li>\n",
    "\t<li>Create a Data Structure and Implement Decision Trees</li>\n",
    "\t<li>Comparing with rpart package.</li>\n",
    "</ol>\n",
    "\n",
    "## Assumptions\n",
    "The implementation will avoid some issues with decision trees. I am going to assume that there never missing values. I will assume that all categorical variables are logical, character, and factor variables. If a variable is numeric, I will assume it is numeric. I am also going to assume that all the data is in a data.frame format.\n",
    "\n",
    "## Data\n",
    "\n",
    "I will be using the Iris dataset for classification and the Boston dataset for regression. Both are available in the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris,load_boston\n",
    "iris = load_iris()\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston Housing Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bostonX = boston['data']\n",
    "bostonY = boston['target']\n",
    "bostonDF = pd.DataFrame(data = np.hstack((bostonX,bostonY.reshape(bostonY.shape[0],1))),\\\n",
    "                        columns=np.append(boston['feature_names'],'PRICE'))\n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   Species  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisX = iris['data']\n",
    "irisY = iris['target']\n",
    "irisDF = pd.DataFrame(data = np.hstack((irisX,irisY.reshape(irisY.shape[0],1))),\\\n",
    "                        columns=np.append(iris['feature_names'],\"Species\"))\n",
    "irisDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripting the First Partition\n",
    "\n",
    "I am going to implement a brute force search for the best partition in the data set. This will be done by going through each value in each feature and finding the feature-value that will best partition the data. To do this we need to implement our metrics I discussed in the theory post:\n",
    "\n",
    "Entropy: Classification\n",
    "Gini Impurity: Classification\n",
    "Variance: Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    if y.size == 0: return 0\n",
    "    p = np.unique(y, return_counts = True)[1].astype(float)/len(y)\n",
    "    return -1 * np.sum(p * np.log2(p+1e-9))\n",
    "\n",
    "def gini_impurity(y):\n",
    "    if y.size == 0: return 0\n",
    "    p = np.unique(y, return_counts = True)[1].astype(float)/len(y)\n",
    "    return 1 - np.sum(p**2)\n",
    "\n",
    "def variance(y):\n",
    "    if y.size == 0: return 0\n",
    "    return np.var(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can quickly calculate these metrics for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.58496249639\n",
      "0.666666666667\n",
      "84.4195561562\n"
     ]
    }
   ],
   "source": [
    "print entropy(irisY)\n",
    "print gini_impurity(irisY)\n",
    "print variance(bostonY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "Now that I have metric functions, I want to calculate the improvement for different partitions. Each partition is going to be calculated using a mask which is a logical vector that states if the row is in the first partition (TRUE) or in the second partition (FALSE). I will call the function information gain, even though it really is only information gain if entropy is the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827861517442\n",
      "0.305906762627\n",
      "0.456884627461\n"
     ]
    }
   ],
   "source": [
    "def information_gain(y,mask,func=entropy):\n",
    "    s1 = np.sum(mask)\n",
    "    s2 = mask.size - s1\n",
    "    if (s1 == 0 | s2 == 0): return 0\n",
    "    return func(y) - s1/float(s1+s2) * func(y[mask]) - s2/float(s1+s2) * func(y[np.logical_not(mask)])\n",
    "\n",
    "\n",
    "print information_gain(irisY,irisX[:,2] < 3.5)\n",
    "print information_gain(irisY,irisX[:,2] < 3.5,gini_impurity)\n",
    "print information_gain(bostonY,bostonX[:,5] < 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Information Gain For A Single Feature\n",
    "\n",
    "I have a function to calculated the information gain for a given criteria, mask, or split, I now will search for the best split among all the values in a given feature. This will give me the best split for that feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 876.5,  458.1,  563.8,  179.8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.apply_along_axis(lambda x: np.sum(x),0,irisX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_numeric': True, 'split_value': 3.0, 'best_change': 0.33333333333333343}\n",
      "{'is_numeric': True, 'split_value': 3.0, 'best_change': 0.91829583213089627}\n",
      "{'is_numeric': True, 'split_value': 1.0, 'best_change': 2.5930420368499885}\n"
     ]
    }
   ],
   "source": [
    "def max_information_gain_split(y,x,func=gini_impurity):\n",
    "    best_change = None\n",
    "    split_value = None\n",
    "    is_numeric = irisX[:,2].dtype.kind not in ['S','b']\n",
    "    \n",
    "    for val in np.unique(np.sort(x)):\n",
    "        mask = x == val\n",
    "        if(is_numeric): mask = x < val\n",
    "        change = information_gain(y,mask,func)\n",
    "        if best_change is None:\n",
    "            best_change = change\n",
    "            split_value = val\n",
    "        elif change > best_change:\n",
    "            best_change = change\n",
    "            split_value = val\n",
    "            \n",
    "    return {\"best_change\":best_change,\\\n",
    "            \"split_value\":split_value,\\\n",
    "            \"is_numeric\":is_numeric}\n",
    "\n",
    "print(max_information_gain_split(irisY,irisX[:,2]))\n",
    "print(max_information_gain_split(irisY,irisX[:,2],entropy))\n",
    "print(max_information_gain_split(bostonY,bostonX[:,3],variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the best split values just happen to be 1 in this case.\n",
    "\n",
    "## Best Feature Split\n",
    "\n",
    "Now I can apply the max_information gain accross all columns and return the information for the best split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 2, 'is_numeric': True, 'split_value': 3.0, 'best_change': 0.33333333333333343}\n",
      "{'index': 2, 'is_numeric': True, 'split_value': 3.0, 'best_change': 0.91829583213089627}\n",
      "{'index': 5, 'is_numeric': True, 'split_value': 6.9429999999999996, 'best_change': 38.220464479057071}\n"
     ]
    }
   ],
   "source": [
    "def best_feature_split(X,y,func=gini_impurity):\n",
    "    best_result = None\n",
    "    best_index = None\n",
    "    for index in range(X.shape[1]):\n",
    "        result = max_information_gain_split(y,X[:,index],func)\n",
    "        if best_result is None:\n",
    "            best_result = result\n",
    "            best_index = index\n",
    "        elif best_result['best_change'] < result['best_change']:\n",
    "            best_result = result\n",
    "            best_index = index\n",
    "    \n",
    "    best_result['index'] = best_index\n",
    "    return best_result\n",
    "\n",
    "print best_feature_split(irisX,irisY)\n",
    "print best_feature_split(irisX,irisY,entropy)\n",
    "print best_feature_split(bostonX,bostonY,variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the best feature-value combination to partition our data we can partition the data into the two data sets. One I will call 'left', and the other I will call 'right'. This will end up being the data structure use in that each node can have two branches: left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_best_mask(X,best_feature_dict):\n",
    "    best_mask = None\n",
    "    if best_feature_dict['is_numeric']:\n",
    "        best_mask = X[:,best_feature_dict['index']] < best_feature_dict['split_value']\n",
    "    else:\n",
    "        best_mask = X[:,best_feature_dict['index']] == best_feature_dict['split_value']\n",
    "    return best_mask\n",
    "\n",
    "bfs = best_feature_split(irisX,irisY)\n",
    "best_mask = get_best_mask(irisX,bfs)\n",
    "left = irisX[best_mask,:]\n",
    "right = irisX[np.logical_not(best_mask),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have scripted a single split, I will now work on creating a tree-based data structure for the decision tree.\n",
    "\n",
    "## Data Structure\n",
    "\n",
    "I am going to make two classes. Decision Tree class that slightly mimics the sklearn API and a Decision Tree Node class will recursively fit the data based on the hyper-parameters we provide. There are many parameters that one could set to control the tree from overfitting. I have have choose: max_depth, min_information_gain, & min_leaf_size. Class diagrams are below.\n",
    "\n",
    "\n",
    "# TO DO - Make Python Diagram\n",
    "[caption id=\"attachment_2649\" align=\"aligncenter\" width=\"768\"]<img class=\"size-full wp-image-2649\" src=\"https://bryantravissmithdotcom.files.wordpress.com/2016/12/decision-tree-algorithm-page-31.png\" alt=\"Class Descriptions of Decision Tree Implementation\" width=\"768\" height=\"448\" /> Class Descriptions of Decision Tree Implementation[/caption]\n",
    "\n",
    " \n",
    "\n",
    "# DecisionTreeNode\n",
    "\n",
    "This is going to be the template I will use for the DecisionTreeNode class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTreeNode(object):\n",
    "    \n",
    "    def __init__(self,\\\n",
    "            X,\\\n",
    "            y,\\\n",
    "            minimize_func,\\\n",
    "            min_information_gain=0.01,\\\n",
    "             max_depth=3,\\\n",
    "             depth=0):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        self.minimize_func=minimize_func\n",
    "        self.min_information_gain=min_information_gain\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = depth\n",
    "        \n",
    "        self.best_split = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.is_leaf = False\n",
    "        self.split_description = \"\"\n",
    "        \n",
    "    def _information_gain(self,mask):\n",
    "        pass\n",
    "    \n",
    "    def _max_information_gain_split(self,X):\n",
    "        pass\n",
    "    \n",
    "    def _best_feature_split(self):\n",
    "        pass\n",
    "    \n",
    "    def _split_node(self):\n",
    "        pass\n",
    "    \n",
    "    def _predict_row(self,row):\n",
    "        pass\n",
    "    \n",
    "    def predict(self,X):\n",
    "        pass\n",
    "    \n",
    "    def __repr__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "This is the same function as I implemented during the scripting section except that it makes reference to the object variables instead of accepting them as parameters.\n",
    "\n",
    "```R\n",
    "information_gain = function(mask){\n",
    "   s1 = sum(mask)\n",
    "   s2 = length(mask)-s1\n",
    "   if ( s1 == 0 | s2 == 0) return(0)\n",
    "   minimize_func(y)-s1/(s1+s2)*minimize_func(y[mask])-s2/(s1+s2)*minimize_func(y[!mask])\n",
    "}\n",
    "```\n",
    "\n",
    "### Max Information Gain Split\n",
    "\n",
    "This function finds the value with the most information gain by a given feature. This is similar to the scripted version above except it also makes reference the object's variables\n",
    "\n",
    "```\n",
    "max_information_gain_split = function(feature){\n",
    "\n",
    "   best_change = NA\n",
    "   split_value = NA\n",
    "   is_numeric = !(is.factor(feature)|is.logical(feature)|is.character(feature))\n",
    "\n",
    "   previous_val <- 0\n",
    "   for( val in sort(unique(feature))){\n",
    "\n",
    "       mask <- feature == val\n",
    "       if (is_numeric) mask <- feature < val\n",
    "       change <- information_gain(mask) if(is.na(best_change) | change > best_change){\n",
    "           best_change = change\n",
    "           split_value = ifelse(is_numeric,\n",
    "                                mean(c(val,previous_val)),\n",
    "                                val)\n",
    "\n",
    "       }\n",
    "       previous_val <- val\n",
    "\n",
    "   }\n",
    "   return(list(\"best_change\"=best_change,\n",
    "               \"split_value\"=split_value,\n",
    "               \"is_numeric\"=is_numeric))\n",
    "}\n",
    "```\n",
    "\n",
    "### Best Feature Split\n",
    "\n",
    "This function finds the feature-value split that gives the most information gain out of the all the features.\n",
    "\n",
    "```\n",
    "best_feature_split = function(){\n",
    "   results <- sapply(x,function(feature) max_information_gain_split(feature))\n",
    "   best_name <- names(which.max(results['best_change',]))\n",
    "   best_result <- results[,best_name]\n",
    "   best_result[[\"name\"]] <- best_name\n",
    "   best_split <<- best_result\n",
    "}\n",
    "```\n",
    "\n",
    "### Best Mask\n",
    "\n",
    "This function finds the mask for the best feature split. This is used to split the node if it is consistent with the hyper-parameters\n",
    "\n",
    "```R\n",
    "best_mask = function(){\n",
    "   best_mask <- x[,best_split$name] == best_split$split_value\n",
    "   if(best_split$is_numeric){\n",
    "       best_mask <- x[,best_split$name] < best_split$split_value\n",
    "   }\n",
    "   return(best_mask)\n",
    "}\n",
    "```\n",
    "\n",
    "### Split Node\n",
    "\n",
    "This function will split the DecisionTreeNode into left and right branches if it is consistent with the hyper-parameters. It is also setting each Node's description so a summary tree can be printed out.\n",
    "\n",
    "```\n",
    "split_node = function() {\n",
    "   if(depth < max_depth){\n",
    "       best_feature_split()\n",
    "       if(best_split$best_change < min_information_gain ){\n",
    "           mask = best_mask()\n",
    "           if(sum(mask) < min_leaf_size && length(mask)-sum(mask) < min_leaf_size){\n",
    "               is_leaf <<- F\n",
    "\n",
    "               branches$left <<- .self$copy()\n",
    "               branches$left$is_leaf <<- T\n",
    "               branches$left$x <<-  branches$left$x[mask,]\n",
    "               branches$left$y <<-  branches$left$y[mask]\n",
    "\n",
    "               branches$left$split_description <<- ifelse(best_split$is_numeric,\n",
    "                                                         paste(c(best_split$name,\n",
    "                                                                 \"<\",\n",
    "                                                                 best_split$split_value),\n",
    "                                                               collapse = \" \"),\n",
    "                                                         paste(c(best_split$name,\n",
    "                                                                 \"=\",\n",
    "                                                                 best_split$split_value),\n",
    "                                                               collapse = \" \"))\n",
    "\n",
    "               branches$left$depth <<-  branches$left$depth+1\n",
    "               branches$left$split_node()\n",
    "\n",
    "               branches$right <<- .self$copy()\n",
    "               branches$right$is_leaf <<- T\n",
    "               branches$right$x <<-  branches$right$x[!mask,]\n",
    "               branches$right$y <<-  branches$right$y[!mask]\n",
    "\n",
    "               branches$right$split_description <<- ifelse(best_split$is_numeric, paste(c(best_split$name, \">=\",\n",
    "                                                                 best_split$split_value),\n",
    "                                                               collapse = \" \"),\n",
    "                                                         paste(c(best_split$name,\n",
    "                                                                 \"!=\",\n",
    "                                                                 best_split$split_value),\n",
    "                                                               collapse = \" \"))\n",
    "\n",
    "               branches$right$depth <-  branches$right$depth+1\n",
    "               branches$right$split_node()\n",
    "           }\n",
    "       }\n",
    "   }\n",
    "   if(is_leaf){\n",
    "       split_description <<- ifelse(identical(minimize_func,variance),\n",
    "                                          paste(c(split_description,\n",
    "                                                  \":\",\n",
    "                                                  \"predict - \",\n",
    "                                                  mean(y)),\n",
    "                                                collapse=\" \"),\n",
    "                                          paste(c(split_description,\n",
    "                                                  \":\",\n",
    "                                                  \"predict - \",\n",
    "                                                  names(which.max(table(y)))),\n",
    "                                                collapse=\" \"))\n",
    "   }\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "### Predict Row\n",
    "\n",
    "This function is used to make a prediction for a single row. It with be used by the predict function.\n",
    "\n",
    "```R\n",
    "predict_row = function(row){\n",
    "   if(is_leaf){\n",
    "       predict_value <- ifelse(identical(minimize_func,variance),\n",
    "                               mean(y),\n",
    "                               names(which.max(table(y))))\n",
    "   } else {\n",
    "       if(best_split$is_numeric){\n",
    "           left = row[best_split$name] < best_split$split_value\n",
    "       } else{\n",
    "           left = row[best_split$name] == best_split$split_value\n",
    "       }\n",
    "       if(left){\n",
    "           predict_value = branches$left$predict_row(row)\n",
    "       } else {\n",
    "           predict_value = branches$right$predict_row(row)\n",
    "       }\n",
    "   }\n",
    "   return(predict_value)\n",
    "}\n",
    "```\n",
    "\n",
    "### Predict\n",
    "\n",
    "This function will predict the values from the fitted tree. If the minimizing function is variance, I assume the problem is a regression problem and return numeric values. If it is not, I assume it is a classification problem and return character values.\n",
    "\n",
    "```R\n",
    "predict = function(features){\n",
    "   pred <- character(length=dim(features)[1])\n",
    "   if(identical(minimize_func,variance)) pred <- numeric(length=dim(features)[1])\n",
    "   for(i in 1:dim(features)[1]){\n",
    "       pred[i] = predict_row(features[i,])\n",
    "   }\n",
    "   pred\n",
    "}\n",
    "```\n",
    "\n",
    "## Final DecisionTreeNode Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTreeNode(object):\n",
    "    \n",
    "    def __init__(self,\\\n",
    "            X,\\\n",
    "            y,\\\n",
    "            minimize_func,\\\n",
    "            min_information_gain=0.001,\\\n",
    "            max_depth=3,\\\n",
    "            min_leaf_size=20,\\\n",
    "            depth=0):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.minimize_func=minimize_func\n",
    "        self.min_information_gain=min_information_gain\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = depth\n",
    "        \n",
    "        self.best_split = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.is_leaf = True\n",
    "        self.split_description = \"root\"\n",
    "        \n",
    "    def _information_gain(self,mask):\n",
    "        s1 = np.sum(mask)\n",
    "        s2 = mask.size - s1\n",
    "        if (s1 == 0 | s2 == 0): return 0\n",
    "        return self.minimize_func(self.y) - \\\n",
    "                s1/float(s1+s2) * self.minimize_func(self.y[mask]) - \\\n",
    "                s2/float(s1+s2) * self.minimize_func(self.y[np.logical_not(mask)])\n",
    "    \n",
    "    def _max_information_gain_split(self,x):\n",
    "        best_change = None\n",
    "        split_value = None\n",
    "        previous_val = None\n",
    "        is_numeric = x.dtype.kind not in ['S','b']\n",
    "\n",
    "        for val in np.unique(np.sort(x)):\n",
    "            mask = x == val\n",
    "            if(is_numeric): mask = x < val\n",
    "            change = self._information_gain(mask)\n",
    "            s1 = np.sum(mask)\n",
    "            s2 = mask.size-s1\n",
    "            \n",
    "            if best_change is None and s1 >= self.min_leaf_size and s2 >= self.min_leaf_size:\n",
    "                best_change = change\n",
    "                split_value = val\n",
    "            elif change > best_change and s1 >= self.min_leaf_size and s2 >= self.min_leaf_size:\n",
    "                best_change = change\n",
    "                split_value = np.mean([val,previous_val])\n",
    "            \n",
    "            previous_val = val\n",
    "\n",
    "        return {\"best_change\":best_change,\\\n",
    "                \"split_value\":split_value,\\\n",
    "                \"is_numeric\":is_numeric}\n",
    "    \n",
    "    def _best_feature_split(self):\n",
    "        best_result = None\n",
    "        best_index = None\n",
    "        for index in range(self.X.shape[1]):\n",
    "            result = self._max_information_gain_split(self.X[:,index])\n",
    "            if result['best_change'] is not None:\n",
    "                if best_result is None:\n",
    "                    best_result = result\n",
    "                    best_index = index\n",
    "                elif best_result['best_change'] < result['best_change']:\n",
    "                    best_result = result\n",
    "                    best_index = index\n",
    "        \n",
    "        if best_result is not None:\n",
    "            best_result['index'] = best_index\n",
    "            self.best_split = best_result\n",
    "    \n",
    "\n",
    "    \n",
    "    def _split_node(self):\n",
    "        \n",
    "        if self.depth < self.max_depth :\n",
    "            \n",
    "            self._best_feature_split() \n",
    "            \n",
    "            if self.best_split is not None and self.best_split['best_change'] >= self.min_information_gain :\n",
    "                   \n",
    "                mask = None\n",
    "                if self.best_split['is_numeric']:\n",
    "                    mask = self.X[:,self.best_split['index']] < self.best_split['split_value']\n",
    "                else:\n",
    "                    mask = self.X[:,self.best_split['index']] == self.best_split['split_value']\n",
    "                \n",
    "                if(np.sum(mask) >= self.min_leaf_size and (mask.size-np.sum(mask)) >= self.min_leaf_size):\n",
    "                    self.is_leaf = False\n",
    "                    \n",
    "                    self.left = DecisionTreeNode(self.X[mask,:],\\\n",
    "                                                self.y[mask],\\\n",
    "                                                self.minimize_func,\\\n",
    "                                                self.min_information_gain,\\\n",
    "                                                self.max_depth,\\\n",
    "                                                self.min_leaf_size,\\\n",
    "                                                self.depth+1)\n",
    "\n",
    "                    if self.best_split['is_numeric']:\n",
    "                        split_description = 'index ' + str(self.best_split['index']) + \" < \" + str(self.best_split['split_value']) + \" ( \" + str(self.X[mask,:].shape[0]) + \" )\"\n",
    "                        self.left.split_description = str(split_description)\n",
    "                    else:\n",
    "                        split_description = 'index ' + str(self.best_split['index']) + \" == \" + str(self.best_split['split_value']) + \" ( \" + str(self.X[mask,:].shape[0]) + \" )\"\n",
    "                        self.left.split_description = str(split_description)\n",
    "\n",
    "                    self.left._split_node()\n",
    "                    \n",
    "                    \n",
    "                    self.right = DecisionTreeNode(self.X[np.logical_not(mask),:],\\\n",
    "                                                self.y[np.logical_not(mask)],\\\n",
    "                                                self.minimize_func,\\\n",
    "                                                self.min_information_gain,\\\n",
    "                                                self.max_depth,\\\n",
    "                                                self.min_leaf_size,\\\n",
    "                                                self.depth+1)\n",
    "                    \n",
    "                    if self.best_split['is_numeric']:\n",
    "                        split_description = 'index ' + str(self.best_split['index']) + \" >= \" + str(self.best_split['split_value']) + \" ( \" + str(self.X[np.logical_not(mask),:].shape[0]) + \" )\"\n",
    "                        self.right.split_description = str(split_description)\n",
    "                    else:\n",
    "                        split_description = 'index ' + str(self.best_split['index']) + \" != \" + str(self.best_split['split_value']) + \" ( \" + str(self.X[np.logical_not(mask),:].shape[0]) + \" )\"\n",
    "                        self.right.split_description = str(split_description)\n",
    "\n",
    "                   \n",
    "                    self.right._split_node()\n",
    "                    \n",
    "        if self.is_leaf:\n",
    "            if self.minimize_func == variance:\n",
    "                self.split_description = self.split_description + \" : predict - \" + str(np.mean(self.y))\n",
    "            else:\n",
    "                values, counts = np.unique(self.y,return_counts=True)\n",
    "                predict = values[np.argmax(counts)]\n",
    "                self.split_description = self.split_description + \" : predict - \" + str(predict)\n",
    "                                          \n",
    "    \n",
    "    def _predict_row(self,row):\n",
    "        predict_value = None\n",
    "        if self.is_leaf:\n",
    "            if self.minimize_func==variance:\n",
    "                predict_value = np.mean(self.y)\n",
    "            else:\n",
    "                values, counts = np.unique(self.y,return_counts=True)\n",
    "                predict_value = values[np.argmax(counts)]\n",
    "        else:\n",
    "            left = None\n",
    "            if self.best_split['is_numeric']:\n",
    "                left = row[self.best_split['index']] < self.best_split['split_value']\n",
    "            else:\n",
    "                left = row[self.best_split['index']] == self.best_split['split_value']\n",
    "                \n",
    "            if left:\n",
    "                predict_value = self.left._predict_row(row)\n",
    "            else:\n",
    "                predict_value = self.right._predict_row(row)\n",
    " \n",
    "        return predict_value\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.apply_along_axis(lambda x: self._predict_row(x),1,X)\n",
    "    \n",
    "    def _rep(self,level):\n",
    "        response = \"|->\" + self.split_description\n",
    "        \n",
    "        if self.left is not None:\n",
    "            response += \"\\n\"+(2*level+1)*\" \"+ self.left._rep(level+1)\n",
    "        if self.right is not None:\n",
    "            response += \"\\n\"+(2*level+1)*\" \"+ self.right._rep(level+1)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self._rep(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Concept on Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "|->root\n",
       " |->index 2 < 2.45 ( 50 ) : predict - 0\n",
       " |->index 2 >= 2.45 ( 100 )\n",
       "   |->index 3 < 1.75 ( 54 )\n",
       "     |->index 2 < 4.45 ( 29 ) : predict - 1\n",
       "     |->index 2 >= 4.45 ( 25 ) : predict - 1\n",
       "   |->index 3 >= 1.75 ( 46 )\n",
       "     |->index 2 < 5.5 ( 20 ) : predict - 2\n",
       "     |->index 2 >= 5.5 ( 26 ) : predict - 2"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtn = DecisionTreeNode(irisX,irisY,gini_impurity)\n",
    "dtn._split_node()\n",
    "dtn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Proof of Concept on ToothGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "|->root\n",
       " |->index 5 < 6.941 ( 430 )\n",
       "   |->index 12 < 14.4 ( 255 )\n",
       "     |->index 12 < 4.97 ( 20 ) : predict - 31.565\n",
       "     |->index 12 >= 4.97 ( 235 )\n",
       "       |->index 12 < 9.715 ( 122 )\n",
       "         |->index 7 < 4.46815 ( 48 ) : predict - 26.4395833333\n",
       "         |->index 7 >= 4.46815 ( 74 ) : predict - 23.0648648649\n",
       "       |->index 12 >= 9.715 ( 113 )\n",
       "         |->index 10 < 17.85 ( 33 ) : predict - 21.8636363636\n",
       "         |->index 10 >= 17.85 ( 80 ) : predict - 20.31875\n",
       "   |->index 12 >= 14.4 ( 175 )\n",
       "     |->index 4 < 0.607 ( 68 )\n",
       "       |->index 0 < 0.55381 ( 39 ) : predict - 19.7384615385\n",
       "       |->index 0 >= 0.55381 ( 29 ) : predict - 15.9517241379\n",
       "     |->index 4 >= 0.607 ( 107 )\n",
       "       |->index 12 < 19.645 ( 51 )\n",
       "         |->index 0 < 5.76921 ( 28 ) : predict - 16.5357142857\n",
       "         |->index 0 >= 5.76921 ( 23 ) : predict - 14.1956521739\n",
       "       |->index 12 >= 19.645 ( 56 )\n",
       "         |->index 0 < 9.87002 ( 23 ) : predict - 12.4652173913\n",
       "         |->index 0 >= 9.87002 ( 33 ) : predict - 9.35454545455\n",
       " |->index 5 >= 6.941 ( 76 )\n",
       "   |->index 5 < 7.437 ( 46 )\n",
       "     |->index 12 < 5.495 ( 23 ) : predict - 35.247826087\n",
       "     |->index 12 >= 5.495 ( 23 ) : predict - 28.9782608696\n",
       "   |->index 5 >= 7.437 ( 30 ) : predict - 45.0966666667"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtn = DecisionTreeNode(bostonX,bostonY,variance,max_depth=5)\n",
    "dtn._split_node()\n",
    "dtn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTree\n",
    "\n",
    "Now that I have implemented the DecisionTreeNode, I can implement the DecisionTree class.  Here is the template code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \n",
    "    def __init__(self,\\\n",
    "            minimize_func,\\\n",
    "            min_information_gain=0.01,\\\n",
    "            max_depth=3,\\\n",
    "            min_leaf_size=20):\n",
    "        \n",
    "        self.root = None\n",
    "        self.minimize_func = minimize_func\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        \n",
    "    def fit(X,y):\n",
    "        pass\n",
    "    \n",
    "    def predict(X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "\n",
    "This is almost identical to the initialization function for DecisionTreeNode. The defaults are only the hyper-parameters though\n",
    "\n",
    "```R\n",
    "initialize = function(...){\n",
    "    defaults <- list(minimize_func=gini_impurity,\n",
    "                     min_information_gain=1e-2,\n",
    "                     min_leaf_size=20,\n",
    "                     max_depth=3,\n",
    "                     root=NULL)\n",
    "\n",
    "    params <- list(...)\n",
    "    fields <- names(getRefClass()$fields())\n",
    "    for( field in fields){\n",
    "        if (!(field %in% names(params))) {\n",
    "            params[[field]] <- defaults[[field]]\n",
    "        }\n",
    "    }\n",
    "    for( param in names(params)){\n",
    "        do.call(\"<<-\",list(param, params[[param]]))\n",
    "    }\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "### Fit\n",
    "\n",
    "The fit function is going to take the features and targets. I am assuming that a data frame will be provided. You can generalize this for other datatypes as you see fit. It just takes the data and features, and creates a new DecisionTreeNode, then splits the node which recursively and greedily builds the tree.\n",
    "\n",
    "```R\n",
    "fit = function(features,target){\n",
    "    root <<- DecisionTreeNode\\$new(x=features,\n",
    "                                y=target,\n",
    "                                minimize_func=minimize_func,\n",
    "                                min_information_gain=min_information_gain,\n",
    "                                min_leaf_size=min_leaf_size,\n",
    "                                max_depth=max_depth\n",
    "                                )\n",
    "    root$split_node()\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "### Predict\n",
    "\n",
    "The predict function is just a wrapper to call the DecisionTreeNode predict function. Right now there are no probabilities. You can add that feature yourself!\n",
    "\n",
    "```R\n",
    "predict = function(features){\n",
    "        root$predict(features)\n",
    "    }\n",
    "```\n",
    "\n",
    "## Fully Implemented Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \n",
    "    def __init__(self,\\\n",
    "            minimize_func,\\\n",
    "            min_information_gain=0.001,\\\n",
    "            max_depth=3,\\\n",
    "            min_leaf_size=20):\n",
    "        \n",
    "        self.root = None\n",
    "        self.minimize_func = minimize_func\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.root =  DecisionTreeNode(X,\\\n",
    "                                    y,\\\n",
    "                                    self.minimize_func,\\\n",
    "                                    self.min_information_gain,\\\n",
    "                                    self.max_depth,\\\n",
    "                                    self.min_leaf_size,\\\n",
    "                                    0)\n",
    "        self.root._split_node()\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.root.predict(X)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.root._rep(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to sklearn\n",
    "\n",
    "Now that we have a decision tree, we can compare the results between the two CART implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|->root\n",
      " |->index 2 < 2.45 ( 50 ) : predict - 0\n",
      " |->index 2 >= 2.45 ( 100 )\n",
      "   |->index 3 < 1.75 ( 54 )\n",
      "     |->index 2 < 4.45 ( 29 ) : predict - 1\n",
      "     |->index 2 >= 4.45 ( 25 ) : predict - 1\n",
      "   |->index 3 >= 1.75 ( 46 )\n",
      "     |->index 2 < 5.5 ( 20 ) : predict - 2\n",
      "     |->index 2 >= 5.5 ( 26 ) : predict - 2\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "root\n",
      "\n",
      " |-> index 2 < 2.45000004768   : predict 0 ( 50.0 )\n",
      " |-> index 2 >= 2.45000004768   \n",
      "   |-> index 3 < 1.75     \n",
      "     |-> index 2 < 4.44999980927       : predict 1 ( 29.0 )\n",
      "     |-> index 2 >= 4.44999980927       : predict 1 ( 25.0 )\n",
      "   |-> index 3 >= 1.75     \n",
      "     |-> index 2 < 5.44999980927       : predict 2 ( 20.0 )\n",
      "     |-> index 2 >= 5.44999980927       : predict 2 ( 26.0 )\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as SklearnDTC\n",
    "from sklearn.tree import DecisionTreeRegressor as SklearnDTR\n",
    "from sklearn.tree import _tree\n",
    "\n",
    "def print_sklearn_tree(tree,index=0,level=0):\n",
    "    response = \"\"\n",
    "    if level == 0:\n",
    "        response += \"root\\n\"\n",
    "    \n",
    "\n",
    "    if tree.feature[index] == -2:\n",
    "        response +=  \": predict \" + str(np.argmax(dt_sklearn.tree_.value[index,0,:])) + \" ( \" +str(np.sum(dt_sklearn.tree_.value[index,0,:])) + \" )\"\n",
    "    else:    \n",
    "        response += \"\\n\"+(2*level+1)*\" \" + \"|-> index \" +  str(tree.feature[index]) + \" < \" + str(tree.threshold[index])\n",
    "        response += (2*(level+1)+1)*\" \"+ print_sklearn_tree(tree,tree.children_left[index],level+1)\n",
    "        response += \"\\n\"+(2*level+1)*\" \" + \"|-> index \" +  str(tree.feature[index]) + \" >= \" + str(tree.threshold[index])\n",
    "        response += (2*(level+1)+1)*\" \"+ print_sklearn_tree(tree,tree.children_right[index],level+1)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "dt_sklearn = SklearnDTC(max_depth=3,min_samples_leaf=20,criterion=\"gini\")\n",
    "dt_sklearn.fit(irisX,irisY)\n",
    "\n",
    "dt_bts = DecisionTree(gini_impurity,min_leaf_size=20,max_depth=3)\n",
    "dt_bts.fit(irisX,irisY)\n",
    "\n",
    "print dt_bts\n",
    "print \"\\n\" + 50*\"-\" + \"\\n\"\n",
    "print print_sklearn_tree(dt_sklearn.tree_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|->root\n",
      " |->index 2 < 2.45 ( 50 ) : predict - 0\n",
      " |->index 2 >= 2.45 ( 100 )\n",
      "   |->index 3 < 1.75 ( 54 )\n",
      "     |->index 2 < 4.95 ( 48 )\n",
      "       |->index 0 < 5.2 ( 5 ) : predict - 1\n",
      "       |->index 0 >= 5.2 ( 43 ) : predict - 1\n",
      "     |->index 2 >= 4.95 ( 6 ) : predict - 2\n",
      "   |->index 3 >= 1.75 ( 46 )\n",
      "     |->index 2 < 5.0 ( 6 ) : predict - 2\n",
      "     |->index 2 >= 5.0 ( 40 ) : predict - 2\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "root\n",
      "\n",
      " |-> index 2 < 2.45000004768   : predict 0 ( 50.0 )\n",
      " |-> index 2 >= 2.45000004768   \n",
      "   |-> index 3 < 1.75     \n",
      "     |-> index 2 < 4.94999980927       \n",
      "       |-> index 0 < 5.14999961853         : predict 1 ( 5.0 )\n",
      "       |-> index 0 >= 5.14999961853         : predict 1 ( 43.0 )\n",
      "     |-> index 2 >= 4.94999980927       : predict 2 ( 6.0 )\n",
      "   |-> index 3 >= 1.75     \n",
      "     |-> index 2 < 4.94999980927       : predict 2 ( 6.0 )\n",
      "     |-> index 2 >= 4.94999980927       : predict 2 ( 40.0 )\n"
     ]
    }
   ],
   "source": [
    "dt_sklearn = SklearnDTC(max_depth=5,min_samples_leaf=5,criterion=\"gini\")\n",
    "dt_sklearn.fit(irisX,irisY)\n",
    "\n",
    "dt_bts = DecisionTree(gini_impurity,min_leaf_size=5,max_depth=5)\n",
    "dt_bts.fit(irisX,irisY)\n",
    "\n",
    "print dt_bts\n",
    "print \"\\n\" + 50*\"-\" + \"\\n\"\n",
    "print print_sklearn_tree(dt_sklearn.tree_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|->root\n",
      " |->index 5 < 6.941 ( 430 )\n",
      "   |->index 12 < 14.4 ( 255 )\n",
      "     |->index 12 < 4.97 ( 20 ) : predict - 31.565\n",
      "     |->index 12 >= 4.97 ( 235 ) : predict - 22.6506382979\n",
      "   |->index 12 >= 14.4 ( 175 )\n",
      "     |->index 4 < 0.607 ( 68 ) : predict - 18.1235294118\n",
      "     |->index 4 >= 0.607 ( 107 ) : predict - 12.9429906542\n",
      " |->index 5 >= 6.941 ( 76 )\n",
      "   |->index 5 < 7.437 ( 46 )\n",
      "     |->index 12 < 5.495 ( 23 ) : predict - 35.247826087\n",
      "     |->index 12 >= 5.495 ( 23 ) : predict - 28.9782608696\n",
      "   |->index 5 >= 7.437 ( 30 ) : predict - 45.0966666667\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "root\n",
      "\n",
      " |-> index 5 < 6.94099998474   \n",
      "   |-> index 12 < 14.3999996185     \n",
      "     |-> index 12 < 4.90999984741       : predict 0 ( 31.565 )\n",
      "     |-> index 12 >= 4.90999984741       : predict 0 ( 22.6506382979 )\n",
      "   |-> index 12 >= 14.3999996185     \n",
      "     |-> index 4 < 0.606999993324       : predict 0 ( 18.1235294118 )\n",
      "     |-> index 4 >= 0.606999993324       : predict 0 ( 12.9429906542 )\n",
      " |-> index 5 >= 6.94099998474   \n",
      "   |-> index 5 < 7.43700027466     \n",
      "     |-> index 12 < 5.49499988556       : predict 0 ( 35.247826087 )\n",
      "     |-> index 12 >= 5.49499988556       : predict 0 ( 28.9782608696 )\n",
      "   |-> index 5 >= 7.43700027466     : predict 0 ( 45.0966666667 )\n"
     ]
    }
   ],
   "source": [
    "dt_sklearn = SklearnDTR(max_depth=3,min_samples_leaf=20)\n",
    "dt_sklearn.fit(bostonX,bostonY)\n",
    "\n",
    "dt_bts = DecisionTree(variance,min_leaf_size=20,max_depth=3)\n",
    "dt_bts.fit(bostonX,bostonY)\n",
    "\n",
    "print dt_bts\n",
    "print \"\\n\" + 50*\"-\" + \"\\n\"\n",
    "print print_sklearn_tree(dt_sklearn.tree_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a Decision Tree class that if the data meets my assumption of not having missing data, is in a data frame format, and classification variables are factors, I can produce results that are similar, if not identical, to the rpart package.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this post, I covered the theory and code of implementing the CART algorithm for producing decision trees and compared the results to rpart on a few toy datasets.  This code is far from production ready, but it does show a high-level implementation of the algorithm.    You can leave any question, comments, corrections, or requestion in the comments below.\n",
    "\n",
    "If you are interested in learning more, I can recommend a few books below.  The image links are affiliate links.   I provide links to free version when available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Science For Business</h3>\n",
    "<a class=\"alignleft\" href=\"https://www.amazon.com/gp/product/1449361323/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1449361323&linkCode=as2&tag=bryantravissm-20&linkId=9299643847c9f603fb0f8e6f3eb9adc4\" target=\"_blank\"><img src=\"//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=US&ASIN=1449361323&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL250_&tag=bryantravissm-20\" border=\"0\" /></a><img class=\"alignleft\" style=\"border: none !important; margin: 0!important;\" src=\"//ir-na.amazon-adsystem.com/e/ir?t=bryantravissm-20&l=am2&o=1&a=1449361323\" alt=\"\" width=\"1\" height=\"1\" border=\"0\" />\n",
    "\n",
    "This book has a good overview of Decision Trees in chapter 3 and just good/general advice about using data science to get practical results.   I gave it a good once over, and continue to reference it from time to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "<h3>Introduction to Statistical Learning</h3>\n",
    "<a class=\"alignleft\" href=\"https://www.amazon.com/gp/product/1461471370/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1461471370&linkCode=as2&tag=bryantravissm-20&linkId=d1c4fec8f4d6f2dc43a3e482e1a23e2c\" target=\"_blank\"><img src=\"//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=US&ASIN=1461471370&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL250_&tag=bryantravissm-20\" border=\"0\" /></a><img class=\"alignleft\" style=\"border: none !important; margin: 0!important;\" src=\"//ir-na.amazon-adsystem.com/e/ir?t=bryantravissm-20&l=am2&o=1&a=1461471370\" alt=\"\" width=\"1\" height=\"1\" border=\"0\" />\n",
    "\n",
    "This is the book I first picked up when I wanted to learn R.  You can google for the accompanying code. It gives a very comprehensive coverage of Statistical Learning and R. I highly recommend it. You can find an older version of this book in a pdf for free: [http://www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Elements of Statistical Learning</h3>\n",
    "<a class=\"alignleft\" href=\"https://www.amazon.com/gp/product/0387848576/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=0387848576&linkCode=as2&tag=bryantravissm-20&linkId=3923e05b895805ecf39befbf4b8ad4fc\" target=\"_blank\"><img class=\"alignleft\" src=\"//ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&MarketPlace=US&ASIN=0387848576&ServiceVersion=20070822&ID=AsinImage&WS=1&Format=_SL250_&tag=bryantravissm-20\" border=\"0\" /></a><img style=\"border: none !important; margin: 0!important;\" src=\"//ir-na.amazon-adsystem.com/e/ir?t=bryantravissm-20&l=am2&o=1&a=0387848576\" alt=\"\" width=\"1\" height=\"1\" border=\"0\" />\n",
    "\n",
    "This is the another book I picked up when I wanted to learn R. There is also code available. This was not a practical approach, but I think it is a good, global experience.  I am amazed that I learn something new every time I open this book. You can find a free older version of this book as well: [http://statweb.stanford.edu/~tibs/ElemStatLearn/](http://statweb.stanford.edu/~tibs/ElemStatLearn/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
